<?xml version="1.0" encoding="UTF-8"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content"><channel><title>Integration Architecture</title><description>Articles by Mike Gough.</description><link>https://mike.gough.me/</link><language>en</language><lastBuildDate>Sun, 16 Aug 2020 02:49:11 +0000</lastBuildDate><pubDate>Sun, 16 Aug 2020 02:49:11 +0000</pubDate><ttl>250</ttl><atom:link href="https://mike.gough.me/feed.rss" rel="self" type="application/rss+xml"/><item><guid isPermaLink="true">https://mike.gough.me/basics/Canonical-data-model</guid><title>Canonical Data Models</title><description>A look at what Canonical Data Models are, including some of their features and examples of situations in which they can be useful.</description><link>https://mike.gough.me/basics/Canonical-data-model</link><pubDate>Sun, 16 Aug 2020 10:00:00 +0000</pubDate><content:encoded><![CDATA[<h2>Canonical Model Design Pattern</h2><p>A <em>Canonical Model</em> is an messaging pattern which provides an additional level of abstraction over data formats which are designed independantly of any one application. This pattern involves creating a messaging or data model that can be leveraged by multiple consumers either directly or indirectly. When new applications or integrations are created, they only need to perform transformations between the <em>Canonical Data Model</em> and their own data formats.</p><p>The <em>Canonical Model</em> design pattern gives you the opertunity to make an integrations between applications easier to understand and maintain. Arguably its most important feature is that it allows for decoupling the underlying data formats of application from one another. It lends itself to being used in situations such as <em>Event Driven</em> architectures where several applications work together through Messaging. It is also often used also in Service Oriented Architectures which promote the reuse of data structures, attributes and data types between applications.</p><h2>Intent</h2><p>This design pattern is used for a variety of reasons including to:</p><ul><li>Standardise data models, helping to minimise dependancies when integrating applications that use different data formats</li><li>Expose reusable services with standardised service contracts</li><li>Reduce the learning curve of developers and simplify the development of integrations between systems</li><li>Reduce the amount of data dormats that data scientists need to learn and work with</li></ul><p>While standardised data models can help to simplify the understanding and implementation of data formats, one drawback is that it is time-consuming and complex to implement as the data model must be designed and produced from scratch. This can become cumbersome and difficult to maintain as the data model grows in size.</p><h2>Problem</h2><p>As with any design pattern its important to understand examples of situations in which they can be useful, so lets look at common example with multiple data sources and consumers.</p><p>An organisation has three applications containing customer data. The first application stores the customers full name. The second stores the customers first name, middle name and last name. The third stores the customers first and last name.</p><p>In this situation each consumer of customer data must translate the three source data formats into their own internal data format. Translation logic is duplicated between consumers and adding new sources becomes a painful process as each consuming application needs to be updated.</p><p>As a result, a <em>Canonical Data Model</em> may be required in order to decouple the data formats and translation that needs to occur between these applications.</p><h2>Summary</h2><p>Canonical Data Models are a design pattern that can be used to simplify the implementation and maintenance cost of integrations between applications. Consumers only need to perform transformations between the <em>Canonical Data Model</em> and their own data formats.</p><p>However it typically involves high upfront design costs and its benefits can't be fully realised unless it is applied to all data consumers and providers. This can be difficult to achieve without buy-in from their respective maintainers or the use of an Enterprise Service Bus.</p>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/basics/Adapter</guid><title>Adapters</title><description>A look at what Adapers are and how they can be used to allow components with incompatible interfaces to collaborate.</description><link>https://mike.gough.me/basics/Adapter</link><pubDate>Tue, 10 Mar 2020 18:00:00 +0000</pubDate><content:encoded><![CDATA[<h2>Adapter Pattern</h2><p>An <em>Adapter</em> is a structural design pattern that allows applications, or components within applications that have incompatible interfaces to collaborate. This pattern can be thought of as a <em>wrapper</em> - as it typically involves wrapping an incompatable interface with a new interface.</p><h2>Intent</h2><p>The intent behind an Adapter is to provide a <em>interface</em> that:</p><ul><li>Acts as translator between your code base and legacy, 3rd party or other interfaces</li><li>Promotes decoupling the client from the conrete implementation of the legacy, 3rd party or other interface which may change over time</li></ul><h2>Problem</h2><p>As with any design pattern its important to understand examples of situations in which they can be useful, so lets look at common example where a consumer requires access to some data but it is not accessible.</p><p>Imagine that you are extending an existing client side web application. The Application needs to fetch transaction data in a JSON format so that it can display it inside of a table for the end user. The data you require is currently stored inside of a relational database, but your browser does not provide a method for you to connect directly to this database and run an SQL query to retrieve the data.</p><p>You could refactor your client side application into a server side application that is capable of connecting to databases and performing SQL queries, however, this introduces extra complexities to your code base and its deployment. It would also mean significantly delaying your delivery time which ultimatly makes the option undesirable.</p><h2>Solution</h2><p>Instead of significantly refactoring your application you decide to create an <em>adapter</em>.</p><p>The adapter acts as a SQL client to the database and invokes SQL queries to fetch the required transactions. It then translates the results of the SQL query into a JSON object that the consumer can understand. This way your client side web application can retrieve the data it needs by making an HTTP call using AJAX to your adapter.</p><p>The diagam below provides a high level example of a protocol adapter, which is a common design pattern found in microservice architectures that allows data to be exposed from legacy systems:</p><img src="https://mike.gough.me//images/posts/adapter-pattern.svg" alt="Adapter pattern"/><p>In this example all the adapter does is transalte the results of the SQL queries into JSON and exposes them to the consuming web application.</p><h2>Summary</h2><p>Adapters are a design pattern that can be used to allow applications to send or recieve data that would otherwise be inaccessible to them, by acting as a midleman that <em>translates</em> between the two.</p><p>An important takeaway is that Adapters are most useful when they are <em>translating</em> from one <em>incompatable interface</em> to a compatible one.</p><p>This pattern may add unnessisary complexity to your code base if you implement it for interfaces which are already compatible or ones you can change (i.e. are not from a 3rd party) - sometimes its just simpler to change your interfaces.</p>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/tips/script-with-go</guid><title>Script with Go</title><description>Learn how to run executable scripts in Go.</description><link>https://mike.gough.me/tips/script-with-go</link><pubDate>Mon, 9 Mar 2020 06:25:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things simple we will assume you have access to a machine running macOS or Linux and have already installed <a href="https://golang.org/">Go</a> and <a href="https://github.com/erning/gorun">Gorun</a>.</p><h2>How to script with Go</h2><img src="https://mike.gough.me//images/posts/go-logo.svg" alt="Go logo"/><p>If you are already a Go developer, you should be familiar with the process of creating a Go package, application or command line tool. Once you have created one of these you would usually need to build a binary and then place it in a executable path before it can be run from a Command Line Interface (CLI). Sometimes, however, we just want to write a script and execute it as is, rather than package it for deployment. This can be particularly useful if you'd like to use Go in different stages of the software development lifecycle such as inside of your Continuous Integration or Continuous Deployment pipelines.</p><p>In such pipelines, it's typical to create a bash file and execute it. For example you could create a file called <code>hello-world</code> with the following contents:</p><pre data-language="bash"><code><span class="hljs-meta">#!/usr/bin/env bash
</span>
<span class="hljs-built_in">echo</span> <span class="hljs-string">"Hello, World!"</span>
</code></pre><p>You would then make it executable by running:</p><pre data-language="bash"><code>chmod +x hello-world
</code></pre><p>And then execute it by running:</p><pre data-language="bash"><code>./hello-world
</code></pre><p>But this is not just limited to runtime languages like bash, the comment on the first line tells the machine what program to use when interpreting the file. This means we can create and execute a script using the Go programming language instead. For example, let's replace the contents of the <code>hello-world</code> file we created earlier with this:</p><pre data-language="go"><code>#! /usr/bin/env gorun

<span class="hljs-keyword">package</span> main

<span class="hljs-keyword">import</span> <span class="hljs-string">"fmt"</span>

<span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">main</span><span class="hljs-params">()</span></span> {
	fmt.Printf(<span class="hljs-string">"hello, world\n"</span>)
}
</code></pre><p>Now you can execute the script by running the same command as we did earlier:</p><pre data-language="bash"><code>./hello-world
</code></pre><p>Once again you should see the text <code>Hello, World!</code> printed to the console.</p><h2>Summary</h2><p>That's it! In this post we have demonstrated a simple way to build small Go scripts that you can modify and run in place whenever you need to, without the need to recompile and deploy them.</p><h2>References</h2><ul><li><a href="https://golang.org/ "golang.org"">Go</a></li><li><a href="https://gist.github.com/posener/73ffd326d88483df6b1cb66e8ed1e0bd "Story: Writing Scripts with Go"">Writing Scripts with Go</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/basics/Facade</guid><title>Facades</title><description>A look at what Facades are and how they can be used to provide a simplified interface to complex libraries and frameworks.</description><link>https://mike.gough.me/basics/Facade</link><pubDate>Fri, 6 Mar 2020 15:00:00 +0000</pubDate><content:encoded><![CDATA[<h2>Facade Pattern</h2><p>A <em>Facade</em> is an design pattern which provides a simplified <em>interface</em> that <em>encapsulates</em> the inernal implementation details of a larger more complex body of code or one which contains lots of moving parts. When working with such code this patttern can be used to <em>seperate concerns</em> from the consumer through a minimalist interface that only exposes the functionality that the consumer really cares about.</p><p>It this way it can thought of as analogous to a shield - as it shields the consumer from the complex details of the code and provides them with a simplified view which is easy to use. This reduces the learning curve required to successfully leverage the code which inturn typically promotes re-use.</p><p>While using the Facade design pattern gives you the opertunity to make an application easier to understand and integrate with, arguably its most important feature is that it promotes decoupling the underlying components of the application from consumers.</p><p>Decoupling your applications code can help you to ensure that it is easy to modify without breaking any consumers which may depend on it. It should be used in situations where consumers need a simple facilitator, one that exposes a simplified interface with a <em>limited</em> set of <em>course-grained</em> behaviors.</p><h2>Intent</h2><p>The intent behind a Facade is to provide a <em>course-grained interface</em> that:</p><ul><li>Offers convenient methods for common tasks that makes the code easier to read, use, understand and test</li><li>Reduces the dependencies from consumer code on the internal implementation components of your code, making it easier to change</li><li>Wraps a collection of poorly designed APIs within a single well-designed API</li></ul><h2>Problem</h2><p>As with any design pattern its important to understand examples of situations in which they can be useful, so lets look at common example where a consumer is trying to access your code remotely.</p><p>Within a single applications code base, fine-graned APIs can provide a developers with a large number of methods which can be chained together in flexible ways to produce new features for customers. However, one of the consequences of such fine-grained behavious is that interactions between components become tightly coupled and typically require lots of method invocations.</p><p>This becomes particularly problematic with remote calls which are often orders of magnitude more expensive to perform since they typically require data to be serialised, authentication and authorisation checks to be performed, packets to be sent accross a network and so on. As a result, a course-grained interface may be required in order to minimise the number of calls needed perform a function.</p><h2>Structure</h2><p>The diagram below provides an example of a coarse-grained interface that's intended to be used by remote consumers that want to minimuse the number of calls needed to get something done:</p><img src="https://mike.gough.me//images/posts/Facade-structure.svg" alt="Facade structure"/><p>In this example all the facade does is transalte the coarse-grained methods it exposes into the underlying fine-grained methods in the Customer object.</p><h2>Related Patterns</h2><p>The <em>Facade</em> pattern can be used to mitigate some of the disadvantages of the <em>canonical data model</em> pattern. It does so by defining simplified interfaces that don't expose canonical models. It acts as a translator which means any changes to the underlying canonical model won’t directly impact the interfaces exposed or their consumers. This can be a great benefit when changes to the canonical model occur frequently or are not in your control.</p><p>One drawback of the <em>Facade</em> pattern in this regard is that can lead to an increased maintenance burden. It is not always practical to wrap and translate the whole canonical model as well as all of an applications underlying dependancies or third party libraries. This is because there is both an upfront cost for wrapping these components as well as a maintenance cost for managing changes for the transformation and integration logic over time.</p><h2>Summary</h2><p>Facades are a design pattern that can be used to simplify access to our code, by pre-defining a <em>limited</em> set of behaviours that can be called by a consumer.</p><p>That's a key part though, because Facades are only realy useful when the underlying code is complex, has many moving parts is difficult to understand or use. It often comes at the expense of limiting the features and flexibility available to consumers due to the maintenance cost of wrapping the underlying components.</p>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/vapor-mongo-api</guid><title>Creating Swift microservices using Vapor and MongoDB</title><description>Learn how to create a server-side RESTful API using Swift and Vapor backed with a NoSQL database for storage and run it all using Docker.</description><link>https://mike.gough.me/articles/vapor-mongo-api</link><pubDate>Sun, 2 Feb 2020 06:00:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things simple we will assume you have access to a machine running macOS or Linux and have already installed Curl, Swift, Vapor and Docker.</p><h2>Creating a MongoDB instance</h2><p>For this tutorial we will need to setup a local MongoDB instance that we can use for our Todo API. The following command starts a MongoDB container instance, allowing us to execute MongoDB statements against a database instance:</p><pre data-language="bash"><code>docker run -d \
  --name mongodb \
  -p 27017-27019:27017-27019 \
  -v ~/data:/data/db \
  mongo:latest
</code></pre><h2>Creating a Vapor project</h2><p>For this post we will make use of the built in Vapor code generator to create a simple RESTful API for creating, reading, updating and deleting todos. Use vapor to create a new project by running the following command:</p><pre data-language="bash"><code>vapor new vapor-todo-api
</code></pre><p>Once the project files have been generated, navigate into the directory by running:</p><pre data-language="bash"><code><span class="hljs-built_in">cd</span> vapor-todo-api
</code></pre><p>In order to connect our Vapor application to a MongoDB database we have to modify some of the generated code to remove the use of SQLite. Let's replace the SQLite dependency and target with MeowVapor inside the <code>Package.swift</code> file:</p><pre data-language="swift"><code><span class="hljs-comment">// swift-tools-version:4.0</span>
<span class="hljs-keyword">import</span> PackageDescription

<span class="hljs-keyword">let</span> package = <span class="hljs-type">Package</span>(
    name: <span class="hljs-string">"vapor-todo-api"</span>,
    products: [
        .library(
            name: <span class="hljs-string">"vapor-todo-api"</span>, 
            targets: [<span class="hljs-string">"App"</span>]
        ),
    ],
    dependencies: [
        <span class="hljs-comment">// 💧 A server-side Swift web framework.</span>
        .package(
            url: <span class="hljs-string">"https://github.com/vapor/vapor.git"</span>, 
            from: <span class="hljs-string">"3.0.0"</span>
        ),

        <span class="hljs-comment">// 🔵 Swift ORM (queries, models, relations, etc) built on Meow, MongoKitten.</span>
        .package(
            url: <span class="hljs-string">"https://github.com/OpenKitten/MeowVapor.git"</span>, 
            from: <span class="hljs-string">"2.1.2"</span>
        )
    ],
    targets: [
        .target(
            name: <span class="hljs-string">"App"</span>, 
            dependencies: [
                <span class="hljs-string">"MeowVapor"</span>, 
                <span class="hljs-string">"Vapor"</span>
            ]
        ),
        .target(
            name: <span class="hljs-string">"Run"</span>, 
            dependencies: [
                <span class="hljs-string">"App"</span>
            ]
        ),
        .testTarget(
            name: <span class="hljs-string">"AppTests"</span>, 
            dependencies: [
                <span class="hljs-string">"App"</span>
            ]
        )
    ]
)
</code></pre><p>MeowVapor is a wrapper for Meow and MongoKitten and provides us with a boiletplate-free object persitance framework for MongoDB and Swift, freeing us from the need to manage our database. After adding this dependancy we need to modify the generated <code>Sources/App/configure.swift</code> file in order to set up the MongoDB driver instead of the default SQLite one. We are going to start by setting the database connection details based on an environment variable <code>MONGODB_URI</code>:</p><pre data-language="swift"><code><span class="hljs-keyword">import</span> MeowVapor
<span class="hljs-keyword">import</span> Vapor

<span class="hljs-comment">// Called before your application initializes.</span>
<span class="hljs-keyword">public</span> <span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">configure</span><span class="hljs-params">(
    <span class="hljs-number">_</span> config: <span class="hljs-keyword">inout</span> Config, 
    <span class="hljs-number">_</span> env: <span class="hljs-keyword">inout</span> Environment, 
    <span class="hljs-number">_</span> services: <span class="hljs-keyword">inout</span> Services)</span></span> <span class="hljs-keyword">throws</span> {
    
    <span class="hljs-keyword">let</span> uri = <span class="hljs-type">Environment</span>.<span class="hljs-keyword">get</span>(<span class="hljs-string">"MONGODB_URI"</span>)
        ?? <span class="hljs-string">"mongodb://localhost/tododb"</span>

    <span class="hljs-comment">// Configure a MongoDB database</span>
    <span class="hljs-keyword">let</span> meow = <span class="hljs-keyword">try</span> <span class="hljs-type">MeowProvider</span>(uri: uri)
    
    <span class="hljs-comment">// Register providers first</span>
    <span class="hljs-keyword">try</span> services.register(meow)

    <span class="hljs-comment">// Register routes to the router</span>
    <span class="hljs-keyword">let</span> router = <span class="hljs-type">EngineRouter</span>.<span class="hljs-keyword">default</span>()
    <span class="hljs-keyword">try</span> routes(router)
    services.register(router, <span class="hljs-keyword">as</span>: <span class="hljs-type">Router</span>.<span class="hljs-keyword">self</span>)

    <span class="hljs-comment">// Register middleware</span>
    <span class="hljs-keyword">var</span> middlewares = <span class="hljs-type">MiddlewareConfig</span>()
    middlewares.use(<span class="hljs-type">ErrorMiddleware</span>.<span class="hljs-keyword">self</span>)
    services.register(middlewares)
}
</code></pre><p>Next, we need to change to the <code>Sources/Models/Todo.swift</code> file so that our Todo model includes an attributed called <code>_id</code> which will be used to store a unique identifier of the type <code>ObjectId</code> for our MongoDB documents:</p><pre data-language="swift"><code><span class="hljs-keyword">import</span> MeowVapor
<span class="hljs-keyword">import</span> Vapor

<span class="hljs-comment">// A single entry of a Todo list.</span>
<span class="hljs-keyword">final</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Todo</span>: <span class="hljs-title">Model</span> </span>{
    
    <span class="hljs-comment">// A unique identifier for the `Todo`</span>
    <span class="hljs-keyword">var</span> _id: <span class="hljs-type">ObjectId</span>

    <span class="hljs-comment">// A title describing what this `Todo` entails.</span>
    <span class="hljs-keyword">var</span> title: <span class="hljs-type">String</span>

    <span class="hljs-comment">// Creates a new `Todo`.</span>
    <span class="hljs-keyword">init</span>(_id: <span class="hljs-type">ObjectId</span>, title: <span class="hljs-type">String</span>) {
        <span class="hljs-keyword">self</span>._id = _id
        <span class="hljs-keyword">self</span>.title = title
    }
    
    <span class="hljs-comment">// Creates a new `Todo`.</span>
    <span class="hljs-keyword">init</span>(title: <span class="hljs-type">String</span>) {
        <span class="hljs-keyword">self</span>._id = <span class="hljs-type">ObjectId</span>()
        <span class="hljs-keyword">self</span>.title = title
    }
}

<span class="hljs-comment">// Allows `Todo` to be encoded to and decoded from HTTP messages.</span>
<span class="hljs-class"><span class="hljs-keyword">extension</span> <span class="hljs-title">Todo</span>: <span class="hljs-title">Content</span> </span>{ }

<span class="hljs-comment">// Allows `Todo` to be used as a dynamic parameter in route definitions.</span>
<span class="hljs-class"><span class="hljs-keyword">extension</span> <span class="hljs-title">Todo</span>: <span class="hljs-title">Parameter</span> </span>{}
</code></pre><p>Then we need to update our routes inside the <code>Sources/App/routes.swift</code> file to support the GET, POST, PUT end DELETE methods for the Todo's endpoint:</p><pre data-language="swift"><code><span class="hljs-keyword">import</span> Vapor
<span class="hljs-keyword">import</span> MeowVapor

<span class="hljs-comment">// Register your application's routes here.</span>
<span class="hljs-keyword">public</span> <span class="hljs-function"><span class="hljs-keyword">func</span> <span class="hljs-title">routes</span><span class="hljs-params">(<span class="hljs-number">_</span> router: Router)</span></span> <span class="hljs-keyword">throws</span> {
    <span class="hljs-keyword">let</span> todoController = <span class="hljs-type">TodoController</span>()
    
    router.<span class="hljs-keyword">get</span>(<span class="hljs-string">"todos"</span>, use: todoController.index)
    router.post(<span class="hljs-string">"todos"</span>, use: todoController.create)
    router.put(<span class="hljs-string">"todos"</span>, use: todoController.upsert)
    router.delete(<span class="hljs-string">"todos"</span>, <span class="hljs-type">Todo</span>.parameter, use: todoController.delete)
}
</code></pre><p>Lastly we will need to build and run our application. To create a Docker image containing our Vapor project, we can use the <code>web.Dockerfile</code> file that was automatically generated for us by running the following command:</p><pre data-language="bash"><code>docker build --build-arg env=docker -t vapor-todo-api-image -f web.Dockerfile
</code></pre><p>This command may take some time to finish, but when complete we will have made a Docker image containing our compiled Vapor project. The container can be run using the command:</p><pre data-language="bash"><code>docker run --name vapor-todo-api -p 8080:80 vapor-todo-api-image
</code></pre><h2>Testing the Todo API</h2><p>Now that we have a running instance of a MongoDB database and our API, let's test it to ensure it works as expected. To insert a new Todo we can call the POST endpoint that we created using the Curl command:</p><pre data-language="bash"><code>curl -X POST \
  -H <span class="hljs-string">"Content-Type: application/json"</span> -d <span class="hljs-string">'{"_id":"5e36366465da966614a18f46", "title":"My todo!"}'</span> \
   localhost/todos
</code></pre><p>After running this command you should recieve a copy of the created Todo in the API response:</p><pre data-language="bash"><code>{<span class="hljs-string">"_id"</span>:<span class="hljs-string">"5e36366465da966614a18f46"</span>, <span class="hljs-string">"title"</span>:<span class="hljs-string">"My todo!"</span>}
</code></pre><p>To verify that our Todo was indeed created, we can use our GET endpoint:</p><pre data-language="bash"><code>curl -X GET localhost/todos
</code></pre><p>This should print the following to the screen:</p><pre data-language="bash"><code>[{<span class="hljs-string">"_id"</span>:<span class="hljs-string">"5e36366465da966614a18f46"</span>,<span class="hljs-string">"title"</span>:<span class="hljs-string">"My Todo!"</span>}
</code></pre><p>Now that we have verified our Todo was indded created, lets replace it using our PUT endpoint by running:</p><pre data-language="bash"><code>curl -X PUT \
  -H <span class="hljs-string">"Content-Type: application/json"</span> -d <span class="hljs-string">'{"_id":"5e36366465da966614a18f46", "title":"My updated todo!"}'</span> \
  localhost/todos
</code></pre><p>You should recieve a copy of the updated Todo in the API response:</p><pre data-language="bash"><code>{<span class="hljs-string">"_id"</span>:<span class="hljs-string">"5e36366465da966614a18f46"</span>, <span class="hljs-string">"title"</span>:<span class="hljs-string">"My updated todo!"</span>}
</code></pre><p>Finally we can delete our Todo using our DELETE endpoint by running:</p><pre data-language="bash"><code>curl -X DELETE localhost/todos/5e36366465da966614a18f48
</code></pre><p>This should return a HTTP 200 response.</p><h2>Summary</h2><p>That's it! In this post we have demonstrated how to build an RESTful API in Swift that allows you to create, read, update and delete Todo's inside of a MongoDB database. Future posts will look at how we can deploy microservices using kubernetes.</p><h2>References</h2><ul><li><a href="https://swift.org/ "swift.org"">Swift</a></li><li><a href="https://vapor.codes "vapor.codes"">Vapor</a></li><li><a href="https://docs.docker.com/get-started/ "Docker"">Docker - Get Started</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/tips/script-with-swift</guid><title>Script with Swift</title><description>Learn how to run executable scripts in Swift.</description><link>https://mike.gough.me/tips/script-with-swift</link><pubDate>Tue, 28 Jan 2020 06:00:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things simple we will assume you have access to a machine running macOS or Linux and have already installed Swift.</p><h2>How to script with swift</h2><img src="https://mike.gough.me//images/posts/swift-logo.svg" alt="Swift logo"/><p>If you are already a Swift developer, you should be familiar with the process of creating a Swift package, application or command line tool. Once you have created one of these you would usually need to build a binary and then place it in a executable path before it can be run from a Command Line Interface (CLI). Sometimes, however, we just want to write a script and execute it as is, rather than package it for deployment. This can be particularly useful if you'd like to use Swift in different stages of the software development lifecycle such as inside of your Continuous Integration or Continuous Deployment pipelines.</p><p>In such pipelines, it's typical to create a bash file and execute it. For example you could create a file called <code>hello-world</code> with the following contents:</p><pre data-language="bash"><code><span class="hljs-meta">#!/usr/bin/env bash
</span>
<span class="hljs-built_in">echo</span> <span class="hljs-string">"Hello, World!"</span>
</code></pre><p>You would then make it executable by running:</p><pre data-language="bash"><code>chmod +x hello-world
</code></pre><p>And then execute it by running:</p><pre data-language="bash"><code>./hello-world
</code></pre><p>But this is not just limited to runtime languages like bash, the comment on the first line tells the machine what program to use when interpreting the file. This means we can create and execute a script using the Swift programming language instead. For example, let's replace the contents of the <code>hello-world</code> file we created earlier with this:</p><pre data-language="swift"><code>#!/usr/bin/env swift

<span class="hljs-built_in">print</span>(<span class="hljs-string">"Hello, World!"</span>)
</code></pre><p>Now you can execute the script by running the same command as we did earlier:</p><pre data-language="bash"><code>./hello-world
</code></pre><p>Once again you should see the text <code>Hello, World!</code> printed to the console.</p><h2>Summary</h2><p>That's it! In this post we have demonstrated a simple way to build small Swift scripts that you can modify and run in place whenever you need to, without the need to recompile and deploy them.</p><h2>References</h2><ul><li><a href="https://swift.org/ "swift.org"">Swift</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/containerise-vapor</guid><title>Containerising a Vapor application</title><description>Learn how to generate and containerise a server-side API using Swift and Vapor inside of a Docker container.</description><link>https://mike.gough.me/articles/containerise-vapor</link><pubDate>Mon, 27 Jan 2020 06:00:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things simple, we will assume you have already installed the following: <em> Docker </em> Swift * Brew</p><p>My setup instructions are based macOS, but you can follow along on Linux as well.</p><h2>What is Vapor?</h2><img src="https://mike.gough.me//images/posts/vapor-logo.svg" alt="Vapor logo"/><p>Vapor is an open source web framework written in Swift. It can be used to create RESTful APIs, web apps and real-time applications using WebSockets. It is particularly geared towards projects that utilise the Model View Controller (MVC) style design pattern.</p><p>To learn more about Vapor, visit <a href="https://vapor.codes">vapor.codes</a></p><h2>Installing Vapor</h2><p>Installing Vapor is a relativly straightforward, it can be done by running the following commands in your preferred Command Line Interface (CLI):</p><pre data-language="bash"><code>brew tap vapor/tap
brew install vapor/tap/vapor
</code></pre><p>Once complete, you can verify that Vapor was installed correctly by running:</p><pre data-language="bash"><code>vapor --<span class="hljs-built_in">help</span>
</code></pre><p>This should print a list of command options to the screen.</p><h2>Creating a Vapor project</h2><p>Vapor is easy to get started with, for this post we will make use of the built in code generator to create a simple Hello World application. Use the vapor executable we installed to create a new project by running the following command:</p><pre data-language="bash"><code>vapor new hello-world
</code></pre><p>Once the project files have been generated, navigate into the directory by running:</p><pre data-language="bash"><code><span class="hljs-built_in">cd</span> hello-world
</code></pre><h2>Creating a Docker Image</h2><p>To create a Docker image containing the Vapor project, we can use the <code></code><code>web.Dockerfile</code><code></code> file that was generated by running the following command:</p><pre data-language="bash"><code>docker build --build-arg env=docker -t vapor-image -f web.Dockerfile
</code></pre><p>This command may take some time to finish, but when complete we will have made a Docker image containing our compiled Vapor project. To run the Docker container and our project we can use the following command:</p><pre data-language="bash"><code>docker run --name vapor-server -p 8080:80 vapor-image
</code></pre><p>Docker will run the container and bind port 80 inside of the container to port 8080 on your local machine. If you go to your browser and enter <a href="http://localhost:8080/hello">http://localhost:8080/hello</a>, you'll see that Vapor is running inside your container... <code>Hello, World!</code></p><h2>Summary</h2><p>Thats It! In this post we have demonstrated how easy it is to get started with Vapor by generating a server-side Swift project. When then illustrated how that project can be compiled and hosted inside of a Docker container. In future posts we will look at how we can expand on this example to develop Swift mircroservices.</p><h2>References</h2><ul><li><a href="https://swift.org/ "swift.org"">Swift</a></li><li><a href="https://vapor.codes "vapor.codes"">Vapor</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/swift-in-a-container</guid><title>How to run Swift in a Docker container</title><description>Learn how to run Swift inside a Docker container. Useful if you don't always have access to a machine running macOS or Linux to compile your code.</description><link>https://mike.gough.me/articles/swift-in-a-container</link><pubDate>Sun, 26 Jan 2020 06:00:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things simple, we will assume you have access to a bash Command Line Interface (CLI) on your local machine. Linux and macOS users should have access to a <em>Terminal</em> application while Windows users will need to install one of several options, including but not limited to:</p><ul><li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">Windows subsystem for Linux</a></li><li><a href="https://gitforwindows.org">Git for Windows (Git Bash)</a></li></ul><p>We will also assume that you have already installed Docker. If you want to get started with Docker on a Windows 10 or Mac OS operating system, installing Docker Desktop is the quickest way.</p><h2>What is Swift?</h2><img src="https://mike.gough.me//images/posts/swift-logo.svg" alt="Swift logo"/><p>Swift is a compiled programming language for writing iOS, macOS, watchOS, tvOS and Linux applications. It was originally created by Apple in 2014 as an open source, type-safe, extensible and fast programming language with a modern syntax.</p><p>To learn more about Swift, visit <a href="https://swift.org">swift.org</a></p><h2>Why run Swift inside docker?</h2><img src="https://mike.gough.me//images/posts/docker-logo.svg" alt="Docker logo"/><p>You can compile and run Swift anywhere that you can install the Swift Compiler. At present this means you either need to have access to a machine running macOS or Linux. For those with a Windows machine, or perhaps working in a corporate environment where you are unable to install the Swift Compiler locally, you can look to Docker for a solution. With Docker we can easily run an official Swift image to allow us to easily compile and run our code in an independent and isolated environment. This can also be useful for portability, as Docker will help us ensure that our Swift code will run the same locally as it would when it's deployed to a server or the cloud.</p><h2>How to run swift in a Docker container?</h2><p>To begin with, we will need a simple Swift file that we can use to compile and run. Following tradition, lets create a file called <code>hello world.swift</code> containing the following:</p><pre data-language="swift"><code><span class="hljs-keyword">import</span> Swift
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Hello, world!"</span>)
</code></pre><p>This should print the words "Hello, world!" on the screen when run.</p><h3>Creating a Swift container</h3><p>Next, we will need to create a file called <code>Dockerfile</code> in our working directory. A Dockerfile has a file format that contains instructions and arguments, which define the contents and startup behaviour of the Docker container. The docker container we will create will run the Swift Compiler ontop of a Linux distribution and allow us to begin programming using Swift even on a Windows machine. To run our simple <code>hello world.swift</code> file, our Dockerfile will need to contain the following:</p><pre data-language="dockerfile"><code><span class="hljs-comment"># The first instruction in a Dockerfile must be FROM, which selects a base image. Since it's recommended to use official Docker images, we will use the official image</span>
<span class="hljs-keyword">FROM</span> swift

<span class="hljs-comment"># Use the WORKDIR instruction to set the working directory, which we will use to store the code we want to run</span>
<span class="hljs-keyword">WORKDIR</span><span class="bash"> /app</span>

<span class="hljs-comment"># Use the ADD instruction to copy our source code from the current directory into the working directory</span>
<span class="hljs-keyword">ADD</span><span class="bash"> . ./</span>

<span class="hljs-comment"># Run the Swift exectutable when the container is started</span>
<span class="hljs-keyword">ENTRYPOINT</span><span class="bash"> [<span class="hljs-string">"swift"</span>]</span>
</code></pre><p>The above Docker file will create an image based on the official Swift Docker image. When the image is built, it will copy the contents of the local directory into a folder named <code>/app</code> inside the image. This allows us to access and run the file inside the container.</p><p>To build an image containing our simple <code>hello world.swift</code> file, we can run the following command:</p><pre data-language="bash"><code>docker build \
  -t my-swift-image .
</code></pre><p>Thats it! We've made a Docker image containing Swift and our <code>hello world.swift</code> file. To run our Docker container and execute the <code>hello world.swift</code> file, run the following command:</p><pre data-language="bash"><code>docker run \
  --rm my-swift-image \
  <span class="hljs-string">"hello world.swift"</span>
</code></pre><p>You should now see <code>Hello, world!</code> printed on the terminal... But wait, doesn't that mean each time we make a change to our code we will have to re-build the docker image? While we certainally could do this, it wouldn't be the most efficient method available to us.</p><h3>Editing Swift code inside of our Docker container on-the-fly</h3><p>Instead of re-building our custom Docker image each time we change our code, we can run it and attach the local directory to it. To demonstrate that this is possible, lets begin with making a small change to our <code>hello world.swift</code> file. Change its contents to the following so that it will print a different line to the screen:</p><pre data-language="swift"><code><span class="hljs-keyword">import</span> Swift
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Hello, cruel world!"</span>)
</code></pre><p>Using the following command to start a container with our custom image, attach the current directory to the <code>/app</code> folder and execute the <code>hello world.swift</code> file we just modified:</p><pre data-language="bash"><code>docker run -it \
  --rm -v $(<span class="hljs-built_in">pwd</span>):/app \
  my-swift-image \
  <span class="hljs-string">"hello world.swift"</span>
</code></pre><p>You should now see <code>Hello, cruel world!</code> printed to the terminal!</p><h3>Using Swift interactivly inside a Docker container</h3><p>Another option would be to use the official Swift image instead of our custom one. To start a container with the official Swift Docker image, you can run the following command:</p><pre data-language="bash"><code>docker run -it \
  --rm -v <span class="hljs-string">"<span class="hljs-variable">$(pwd)</span>"</span>:/app \
  swift
</code></pre><p>This will create a temporary container running Swift in interactive mode, allowing you to execute Swift commands inside a terminal environment. Lets tell Swift to execute our <code>hello world.swift</code> file by using the following command:</p><pre data-language="bash"><code>swift run <span class="hljs-string">"/app/hello world.swift"</span>
</code></pre><p>Once again you should see <code>Hello, cruel world!</code> printed to the terminal.</p><h2>Summary</h2><p>Thats it! In this post we have demonstrated how easy it is to run swift code inside a Docker container. In future posts we will look at how we can containerise and run server-side Swift applications.</p><h2>References</h2><ul><li><a href="https://swift.org/ "swift.org"">Swift</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/tips/books</guid><title>Recommended books</title><description>Great books for learning more about development and integration.</description><link>https://mike.gough.me/tips/books</link><pubDate>Sat, 14 Dec 2019 06:00:00 +0000</pubDate><content:encoded><![CDATA[<h2>Fiction</h2><p>The below books portray science and technology in a very idealised way and might use a bit of artistic license to keep readers engaged. They can be good to take you back to your roots, give you a new perspective or just for some quick entertainment.</p><ul><li><a href="https://store.xkcd.com/pages/if-you-re-looking-for-the-what-if-book">What If? Serious Scientific Answers to Absurd Hypothetical Questions</a>: If you've ever wanted answers to questions such as "How much computing power could we could achieve if the entire world population stopped whatever we are doing right now and started doing calculations? How would it compare to a modern-day computer or smartphone?" then this book is for you.</li></ul><h2>Non-Fiction</h2><p>These books are great when it comes to self-learning and improving your own skillset.</p><ul><li><a href="http://wiki.c2.com/?DesignPatternsBook">Design Patterns: Elements of Reusable Object-Oriented Software</a>: Otherwise known as the Gang of Four book, this is hands down the best book ever written on object-oriented design and has been very influential in the software development field. This book is a great introduction to software design and architecture for developers.</li><li><a href="https://martinfowler.com/books/refactoring.html">Refactoring</a>: This book by Martin Fowler and Kent Beck acts as a guide to making decisions around when and how to refactor code to keep it cheap and easy to modify to meet future needs.</li><li><a href="https://agilemanifesto.org/principles.html">Principles behind the Agile Manifesto</a>: I still find it amazing the amount of development teams that I come across who claim to be following an Agile Methodology but who havent read the twelve principles behind the Agile Manifesto. I recommend this short read to anyone working in an agile team, particularly those having dificulties achieving results.</li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/tips/git-aliases</guid><title>Git Aliases</title><description>In this post we will walk through how you can make your git workflow experience simpler and easier with aliases.</description><link>https://mike.gough.me/tips/git-aliases</link><pubDate>Fri, 13 Dec 2019 06:00:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things brief, we will assume you already have Git installed and setup correctly.</p><h2>What is a Git Alias?</h2><p>An alias in Git is synonomous with a shortcut and is similar to <em>aliases</em> found in command processors such as <em>Bash</em>. They can be used to map long, complicated or hard to remember commands into shorter and easier to remember ones that require fewer keystrokes. For example, consider the <code>git push</code> command. It is frequently used to check in your staged changes, but instead of typing in the word <code>commit</code> an alias could be created to shorten it to <code>c</code> allowing <code>git c</code> to be typed instead. This can be a life changer for longer commands which can be particularly verbose.</p><h2>How to create a Git Alias</h2><p>Git Aliasss can be created using the <code>git config</code> command in either a local or global scope. To create an alias for commit which you can use in any repository, you can run the following command in your preferred Command Line Interface (CLI):</p><pre data-language="bash"><code>git config --global alias.c commit
</code></pre><p>The previous example can also be modified to use the <code>--local</code> flag instead of <code>--global</code> and run from inside of a repository to create a shortcut for just that repository instead any repository, like so:</p><pre data-language="bash"><code>git config --<span class="hljs-built_in">local</span> alias.c commit
</code></pre><p>When the <code>config</code> command was run with the <code>--global</code> flag, Git created or modified the <code>.gitconfig</code> file, which is a hidden file typically located in your User home directory at <code>~/.gitconfig</code><code></code>. If you were to inspect the contents of this file, you would see an alias section that looks something like this:</p><pre data-language="plaintext"><code>[alias]
  c = commit
</code></pre><p>This file can also be edited direcly to add, remove or updatd aliases for advanced users who are familiar with the correct syntax.</p><h2>Useful Git Aliases</h2><p>Here are a few aliases which I have found improved my own git workflow experience:</p><h3>1. Inline Quick Commit</h3><p>Sometimes you just want to stage and commit all of the files you have modified in the current repository. This command quickly adds the files and commits them:</p><pre data-language="bash"><code>git config --global alias.coi <span class="hljs-string">"!git add -A &amp;&amp; git commit -m "</span>
</code></pre><p>Which can be run using:</p><pre data-language="bash"><code>git coi &lt;commit message&gt;
</code></pre><h3>2. Amend the last commit</h3><p>As developers, we are often optsmistic about our code and have a tendancy to commit our work only to realise we missed a file or forgot to remove a debugging statement. For these occasions, the following alias will ammend the latest commit with all of the changed files in the local repository, without the need to supply a new commit message:</p><pre data-language="bash"><code>git config --global alias.coa <span class="hljs-string">"!git add -A &amp;&amp; git commit --amend --no-edit"</span>
</code></pre><p>The alias can be run using:</p><pre data-language="bash"><code>git coa
</code></pre><h3>3. Push to the remote branch</h3><p>In my git workflow I typically create a branch locally, work on a feature and then want to push my completed work to origin either at the end of the day or once complete (whichever comes first). When I do so for the first time, a branch with the same name does not yet exist on origin so git throws an error. To address this, I use the following alias:</p><pre data-language="bash"><code>git config --global alias.po <span class="hljs-string">"!git push origin <span class="hljs-variable">$(git rev-parse --abbrev-ref HEAD)</span>"</span>
</code></pre><p>Which can be run using:</p><pre data-language="bash"><code>git po
</code></pre><h3>4. Update Submodules</h3><p>Some of the Git repositories I work with contian one or more nested submodules, to update them I make use of the following alias:</p><pre data-language="bash"><code>git config --global alias.usr <span class="hljs-string">"!git submodule update --init --recursive"</span>
</code></pre><p>This alias can be run using:</p><pre data-language="bash"><code>git usr
</code></pre><h2>Summary</h2><p>We have previously walked through how to setup git hooks that run test suites and now we've demonstrated how easy it is to simplify your git workflow and create aliases for those commands you use frequently or are hard to remember.</p><h2>References</h2><ul><li><a href="https://git-scm.com/book/en/v2/Git-Basics-Git-Aliases "Git Basics - Git Aliases"">Git Basics - Git Aliases</a></li><li><a href="https://mike.gough.me/posts/git-hooks.html "Git pre-commit hook"">Git Hooks</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/containerising-bat-test</guid><title>Black-box testing with MuleSoft's Blackbox Automated Testing (BAT) CLI</title><description>In this post we will walk through how you can run a simple test using the BAT CLI inside of a Docker container.</description><link>https://mike.gough.me/articles/containerising-bat-test</link><pubDate>Sun, 23 Jun 2019 06:00:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things brief, we will assume you already have Docker installed and setup correctly.</p><h2>What is the BAT CLI?</h2><img src="https://mike.gough.me//images/posts/mulesoft-logo.svg" alt="MuleSoft logo"/><p>The BAT CLI is an API Functional Monitoring tool produced by MuleSoft for assuring the quality and reliablility of APIs. It provides a convenient method for those working in the MuleSoft technology stack to implement Black-box testing and Runtime monitoring. Using dataweave, it is possible to develop tests that validate the behavior of APIs against live upstream systems, based on inputs and outputs. Additionally, it has a monitoring capability that allows you to verify that deployed APIs are operating as expected and output test results in a variety of formats as a once off operation or on a schedule.</p><h2>Test Writing Language</h2><p>In this post, we will be writing a test manually and running it as a once off operation with the BAT CLI. To begin with, we will need to create file called <code>tests/example.dwl</code> in our working directory. As the <code>dwl</code> file extension indicates, our test will be written using the Dataweave language. Dataweave is an expression language introduced by MuleSoft for transforming data. Within Dataweave, we will use an embeded Domain Specific Language (DSL) called Behaviour Driven Developmenmt (BDD), which has a similar syntax to other testing frameworks, to define our test. The following example shows a typical API test written using BDD in Dataweave, add the example to the <code>tests/example.dwl</code> file:</p><pre data-language="plaintext"><code>%dw 2.0 // Dataweave 2.0
// Behaviour Driven Development (BDD) Domain Spesific Langage (DSL)
import * from bat::BDD 
// Common matchers (i.e. mustEqual, mustMatch, every, oneOf, assert, etc.)
import * from bat::Assertions
---
// Defines a suite of related tests
suite("Example") in [

  // The result of the test must be 200 to be considered a success
  it must 'return 200' in [

    // Perform a GET request
    GET `$(config.base_url)/zen` with {} assert [
      
      // Assert that the HTTP response code recieved was 200
      $.response.status mustEqual 200
    ] 
  ]
]
</code></pre><p>The <code>$(config.base_url)</code> section from the above example indicates that the test is expecting us to provide the Base URL as a configuration item, rather than hard code it within the test. Building our test in this way gives us the ability to change the Base URL so that our test can be executed against different environments. To store our config, we will need to create a <code>config</code> folder and add some configuration files to it. Lets create a file called <code>config/default.dwl</code> with the following contents:</p><pre data-language="plaintext"><code>%dw 2.0 // Dataweave 2.0
---
config::local::main({})
</code></pre><p>Next, create a config file for our production configuration called <code>config/prod.dwl</code> with the following contents:</p><pre data-language="plaintext"><code>%dw 2.0
---
{
  base_url: 'https://api.github.com'
}
</code></pre><p>Then, we need to create a manifest file called <code>bat.yaml</code> which will define the tests to be run as well as the type and location of the report to be produced by BAT. The file should have the following contents:</p><pre data-language="yaml"><code><span class="hljs-comment"># Name of the test suite</span>
<span class="hljs-attr">suite:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">"Example Test Suite"</span>

<span class="hljs-comment"># A list of tests to be executed</span>
<span class="hljs-attr">files:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">file:</span> <span class="hljs-string">./tests/example.dwl</span>

<span class="hljs-comment"># A list of reports to be produced</span>
<span class="hljs-attr">reporters:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">HTML</span>
    <span class="hljs-attr">outFile:</span> <span class="hljs-string">/usr/src/mymaven/results.html</span>
</code></pre><p>Your working directory should now contain the following file structure:</p><pre data-language="plaintext"><code>working-directory
├── bat.yaml
├── tests
│   └── example.dwl
└── config
    ├── default.dwl
    └── prod.dwl
</code></pre><h2>Validating a Test Suite</h2><p>To validate the files and folders we have created, we can run BAT inside a Docker container and provide some additional command line options. From the base of the working directory, run the following command:</p><pre data-language="bash"><code>docker run --init --rm \
  -v <span class="hljs-string">"<span class="hljs-variable">${PWD}</span>"</span>:/usr/src/mymaven mikeyryan/mule-blackbox-automated-testing:latest \
  bat.yaml --config=prod --validate
</code></pre><p><code>bat.yaml</code> tells BAT the directory the location of the manifest file. <code>--config=prod</code> selects a configuration file named prod (from the config folder) and registers the result as a global variable for interpolation by our tests</p><h2>Running a Test Suite</h2><p>To execute the tests defined in the manifest file, we can run BAT inside a Docker container using the following command:</p><pre data-language="bash"><code>docker run --init --rm \
  -v <span class="hljs-string">"<span class="hljs-variable">${PWD}</span>"</span>:/usr/src/mymaven mikeyryan/mule-blackbox-automated-testing:latest \
  bat.yaml --config=prod
</code></pre><p>The output of the command should match the below:</p><pre data-language="bash"><code>BAT Version: 1.0.96
<span class="hljs-comment">#  File: ./tests/example.dwl</span>
    
    Example
        
        <span class="hljs-built_in">return</span> 200
          ✓ GET https://api.github.com/zen (981.18ms)
            ✓ 200 must equal 200
</code></pre><h2>Summary</h2><p>We have previously walked through how to containerise BAT and now we've demonstrated how easy it is to develop and run BAT tests inside Docker containers.</p><h2>References</h2><ul><li><a href="https://hub.docker.com/r/mikeyryan/mule-blackbox-automated-testing "mikeyryan/mule-blackbox-automated-testing"">Docker Hub - MuleSoft docker images</a></li><li><a href="https://docs.mulesoft.com/api-functional-monitoring/bat-install-task "BAT Installation"">Mulesoft - Install BAT</a></li><li><a href="https://docs.mulesoft.com/api-functional-monitoring/bat-command-reference "BAT CLI Reference"">BAT CLI Reference</a></li><li><a href="https://docs.mulesoft.com/api-functional-monitoring/bat-bdd-reference "BAT BDD Reference"">BAT BDD Reference</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/containerising-mule-bat</guid><title>Containerising MuleSoft's Blackbox Automated Testing (BAT) CLI tool</title><description>In this post we will assume that you have Docker and would like to create an image that contains the Blackbox Automated testing (BAT) application published by Mulesoft.</description><link>https://mike.gough.me/articles/containerising-mule-bat</link><pubDate>Sat, 22 Jun 2019 10:19:00 +0000</pubDate><content:encoded><![CDATA[<p>If you're looking for a Mule Docker image you can use without making your own, then you can check out <a href="https://hub.docker.com/r/mikeyryan/mule-blackbox-automated-testing">mikeyryan/mule-blackbox-automated-testing</a> on Docker Hub.</p><h2>What is BAT?</h2><img src="https://mike.gough.me//images/posts/mulesoft-logo.svg" alt="MuleSoft logo"/><p>The BAT CLI is an API Functional Monitoring tool produced by MuleSoft for assuring the quality and reliablility of APIs. It provides a convenient method for those working in the MuleSoft technology stack to implement Black-box testing and Runtime monitoring. Using dataweave, it is possible to develop tests that validate the behavior of APIs against live upstream systems, based on inputs and outputs. Additionally, it has a monitoring capability that allows you to verify that deployed APIs are operating as expected and output test results in a variety of formats as a once off operation or on a schedule.</p><h2>Why Containerise BAT?</h2><p>BAT will run perfectly fine anywhere that you can install Java. However, you may find yourself in need of a solution that's more scalable than installing it on a standalone machine or needing to run multiple versions it at the same time. In fact, a typical scenario for using BAT is as part of a Continuous Integration (CI) and Continuous Deployment (CD) process to test your API before pushing code into a shared repository and/or verifying that a deployment to production has worked correctly. With Docker we can easily package BAT in a container which is useful for portability, as Docker will help ue sneusre that our tests will run the same locally as they will when deployed to a server or used as part of a CI/CD process.</p><h2>Creating a Docker image</h2><img src="https://mike.gough.me//images/posts/docker-logo.svg" alt="Docker logo"/><p>To begin with, we will need to create file called <code>Dockerfile</code> in our working directory. A Dockerfile has a file format that contains instructions and arguments, which define the contents and startup behaviour of the Docker container. To run BAT, our Dockerfile will need to contain the following:</p><pre data-language="dockerfile"><code><span class="hljs-comment"># The first instruction in a Dockerfile must be FROM, which selects a base image. Since it's recommended to use official Docker images, we will use the official image for maven.</span>
<span class="hljs-keyword">FROM</span> maven:<span class="hljs-number">3.6</span>-jdk-<span class="hljs-number">8</span>

<span class="hljs-keyword">LABEL</span><span class="bash"> maintainer=<span class="hljs-string">"https://mike.gough.me"</span></span>

<span class="hljs-comment"># Install BAT using the script provided by MuleSoft</span>
<span class="hljs-keyword">RUN</span><span class="bash"> curl -o- <span class="hljs-string">'https://s3.amazonaws.com/bat-wrapper/install.sh'</span> | bash</span>

<span class="hljs-comment"># Set the workdir to the default for the maven image we are using</span>
<span class="hljs-keyword">WORKDIR</span><span class="bash"> /usr/src/mymaven</span>

<span class="hljs-comment"># Start BAT when the container is run</span>
<span class="hljs-keyword">ENTRYPOINT</span><span class="bash"> [<span class="hljs-string">"bash"</span>, <span class="hljs-string">"bat"</span>]</span>
</code></pre><p>The above Docker file will create an image based on the official maven Docker image. It downloads and installs the latest version of BAT. To create the Docker image with the latest version of BAT, run the following command:</p><pre data-language="bash"><code>docker build mule-blackbox-automated-testing:latest .
</code></pre><p>Thats it! In a future post we will look at how we can use our BAT container to test an API.</p><h2>References</h2><ul><li><a href="https://hub.docker.com/r/mikeyryan/mule-blackbox-automated-testing "mikeyryan/mule-blackbox-automated-testing"">Docker Hub - MuleSoft docker images</a></li><li><a href="https://docs.mulesoft.com/api-functional-monitoring/bat-install-task "BAT Installation"">Mulesoft - Install BAT</a></li><li><a href="https://github.com/Mike-Gough/mule-blackbox-automated-testing "BAT Dockerfile source code"">GitHub - Source code</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/linux-subsystem-for-windows</guid><title>Setup Subsystem for Linux on Windows 10</title><description>Windows Subsystem for Linux (WSL) lets developers run GNU/Linux code side-by-side with Windows processes. In this post we will walk through how you can setup WSL.</description><link>https://mike.gough.me/articles/linux-subsystem-for-windows</link><pubDate>Sat, 8 Jun 2019 14:20:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things simple, we will assume you have the following:</p><ul><li>A Windows 10 Machine with build 16215 or later</li><li>An administrator account</li><li>A Windows Store account</li></ul><h2>What is WSL?</h2><p>WSL is an optional feature of Windows 10 that allows Linux programs to run nativly on Windows. It was originally desiged by Microsoft in partnetship with Canonical (the creators of Ubuntu) and provides an environment that looks and behaves just like Linux. At a high level this allows you to run Linux code without having to fire up a Virtual Machine.</p><h2>Why WSL?</h2><p>WSL gives Windows users access to powerful Linux applications and GNU tools such as find, awk, sed and grep. Although not all Linux applications can be used, it comes with software package tools such as apt and dpkg which can be used to install applications. WSL shows its utility in Continuous Integration and Continuous Delivery environmens that make use of open source software, as many open source tools and libraries assume developers are using Linux. While tools such as Docker help to ensure developed applications can be run consistently, WSL helps to ensure they can be developed consistently.</p><h2>Enable WSL</h2><p>Begin by opening PowerShell as an Administrator and running the following command:</p><pre data-language="powershell"><code><span class="hljs-built_in">Enable-WindowsOptionalFeature</span> <span class="hljs-literal">-Online</span> <span class="hljs-literal">-FeatureName</span> Microsoft<span class="hljs-literal">-Windows</span><span class="hljs-literal">-Subsystem</span><span class="hljs-literal">-Linux</span>
</code></pre><p>Then restart your computer if prompted.</p><h2>Install your Linux Distribution of Choice</h2><p>To download and install your preferred distribution of linux, open the Windows Store and search for <code>WSL</code> or click on one of the links below:</p><ul><li><a href="https://www.microsoft.com/store/p/ubuntu/9nblggh4msv6">Ubuntu</a></li><li><a href="https://www.microsoft.com/store/apps/9njvjts82tjx">OpenSUSE</a></li><li><a href="https://www.microsoft.com/store/apps/9p32mwbh6cns">SLES</a></li><li><a href="https://www.microsoft.com/store/apps/9PKR34TNCV07">Kali Linux</a></li><li><a href="https://www.microsoft.com/store/apps/9MSVKQC78PK6">Debian GNU/Linux</a></li></ul><p>On the distributions page, click the <em>Get</em> button. After pressing get, you may find you need to hit an <em>Install</em> button. Once your Linux distribution is installed, you must initialize it before it can be used.</p><h2>Initialising a new Linux Distribution</h2><p>To complete the initialisation of your newly installed distribution,you'll need to launch an instance of it. You can do this by clicking the <em>Launch</em> button inside the Windows Store, or by launching it from the Start menu. The first time a newly installed distribution launches, a Console window will open and you'll be asked to wait for the installation to complete.</p><p>Once the distributions files are de-compressed and stored on your PC, you'll be prompted to setup a new Linux user account. When prompted, enter a new UNIX username and password. The user account that will created is a normal (non-administrator) user that you'll be logged-in as by default when launching the discribution. Make sure to chose a password you will remember, as when elevating your prilidges (using sudo), you will need to enter your password.</p><h2>Updating and upgrading your distributons packages</h2><p>Most distributions ship with an minimal packages to keep the initial download size small. Microsoft recommend regularly updating your package catalog, and upgrading your installed packages using the distributions preferred package manager:</p><ul><li>On Debian/Ubuntu, you use apt: <code>sudo apt update &amp;&amp; sudo apt upgrade</code></li><li>On OpenSuse, you use zypper: <code>sudo zypper refresh &amp;&amp; sudo zypper update</code></li></ul><h2>Summary</h2><p>We have walked through how to enable WSL, install a Linux Distribution and ensure it is up to date. You should now have a working Linux Distribution on your Windows 10 Machine that you can use to run bash scrips and much more.</p><h2>References</h2><ul><li><a href="https://docs.microsoft.com/en-us/windows/wsl/about "About the Windows Subsystem for Linux"">About the Windows Subsystem for Linux</a></li><li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10 "Install on Windows 10"">Install on Windows 10</a></li><li><a href="https://docs.microsoft.com/en-us/windows/wsl/initialize-distro "Initialize distro"">Initialize distro</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/kubernetes-dashboard-deployment</guid><title>Using Kubernetes dashboard to deploy a Mule 4 application</title><description>Once you have a running Kubernetes cluster, you can deploy your containerised applications on top of it. In this post we will walk through how you can deploy a containerised mule application using the Kubernetes dashboard.</description><link>https://mike.gough.me/articles/kubernetes-dashboard-deployment</link><pubDate>Thu, 30 May 2019 06:10:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things simple, we will assume you have already setup a local single node Kubernetes cluster and deployed its dashboard.</p><h2>Create a new Kubernetes deployment</h2><img src="https://mike.gough.me//images/posts/kubernetes-logo.svg" alt="Kubernetes logo"/><p>Begin by using your browser to navigate to the Kubernetes dashboard running on your local machine <a href="http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a>. Use the menu to the left of the page to open the <em>Deployments</em> screen:</p><img src="https://mike.gough.me//images/posts/kubernetes-deployments.jpg" alt="Deployments screen"/><p>Then click the <em>Create</em> button in the top right hand corner. In the Wizard that appears, select the <em>Create an App</em> tab and provide the following values:</p><ul><li>app name: mule-4-hello-world</li><li>container image: mikeyryan/mule-4-hello-world:ce</li><li>number of pods: 1</li><li>service: External</li><li>port: 8081</li><li>target port: 8081</li><li>protocol: TCP</li></ul><p>If you've been following on from earlier walkthroughs, or have your own containerised Mule application, feel free to replace the value for the container image. The image <code>mikeyryan/mule-4-hello-world:ce</code> is a a simple Hello World application for Mule 4. It has a single HTTP listener flow that listens on <code>http://localhost:8081/api/hello-world</code> and can be found on <a href="https://github.com/Mike-Gough/mule-4-hello-world">GitHub</a>.</p><img src="https://mike.gough.me//images/posts/kubernetes-deployment-create-an-app.jpg" alt="Create an app"/><p>Finally, click the <em>Deploy</em> button. Once the deployment starts we can see the deployment name in the Dashboard on the Deployments screen:</p><img src="https://mike.gough.me//images/posts/kubernetes-deployments-with-mule-4-hello-world.jpg" alt="Deployments deployment screen with a mule app"/><p>To verify that the application has started successfully, you can use the menu to the left of the page to open the <em>Pods</em> screen:</p><img src="https://mike.gough.me//images/posts/kubernetes-pods.jpg" alt="Kubernetes pods screen with a mule pod"/><p>And click the circled icon to inspect the logs for the running mule-4-hello-world Docker container. You should see a screen similar to this:</p><img src="https://mike.gough.me//images/posts/kubernetes-pods-log.jpg" alt="Kubernetes pods screen with the logs for a mule pod"/><h2>Testing the application</h2><p>Now that we have verified that the application has been deployed successfully, we are ready to test it. The containerised <code>mule-4-hello-world</code> application exposes an API endpoint on port 8081. When deploying the example using Kubernetes we chose to leave the port the same, but make it externally accessible. As such the application can be accessed by navigating to <a href="http://localhost:8081/api/hello-world">http://localhost:8081/api/hello-world</a> in your browser. When accessed, your browser should show:</p><pre data-language="plaintext"><code>Hello from Mule 4.2.0
</code></pre><h2>Summary</h2><p>We have previously walked through how to containerise the Mule ESB as well as a Mule 4 application. Now we've demonstrated how easy it is deploy a containerised Mule 4 application using the Kubernetes dashboard. In future posts we will look at how this can be achieved using Kubectl.</p><h2>References</h2><ul><li><a href="https://kubernetes.io/docs/tutorials/kubernetes-basics/ "Kubernetes Basics"">Kubernetes Basics</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/containerising-mule-esb-applications</guid><title>Containerising a Mule 4 application</title><description>In this post we will walk through how you can run a simple Mule 4 application inside of a Docker container.</description><link>https://mike.gough.me/articles/containerising-mule-esb-applications</link><pubDate>Tue, 28 May 2019 13:29:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things brief, we will assume you already have JDK 8, Maven 3 and Docker installed and setup correctly. Your docker host needs to have at least 1GB of available RAM to run Mule ESB Server Runtime. You can refer to the <a href="https://docs.mulesoft.com/mule-runtime/4.2/hardware-and-software-requirements">Mule ESB hardware requirements</a> documentation for additional information.</p><h2>Hello world Mule 4 application</h2><img src="https://mike.gough.me//images/posts/mulesoft-logo.svg" alt="MuleSoft logo"/><p>I have created a simple Hello World application inside Mule 4 which can be used for this walkthrough. It has a single HTTP listener flow that listens on <code>http://localhost:8081/api/hello-world</code> and can be found on <a href="https://github.com/Mike-Gough/mule-4-hello-world">GitHub</a>. The GitHub repository contains an example for both the community edition and enterprise edition of Mule. You can download and use either example, or if you prefer, your own application. However, if you choose to use the enterprise edition inside Docker you may need to install a licence file.</p><h2>Building the application</h2><p>To build the application, open your Command Line Interface (CLI) of choice, navigate to the application directory and run:</p><pre data-language="bash"><code>mvn clean package
</code></pre><p>This will cause Maven to create a <code>target</code> folder and package the application into a jar file inside of it. We will be using this jar file inside of our Docker image.</p><h2>Creating a Docker image for the Mule application</h2><p>To begin with, we will need to create a new file called <code>Dockerfile</code> in our working directory. A Dockerfile has a file format that contains instructions and arguments, which define the contents and startup behaviour of the Docker container. To containerise the example Mule application, our Dockerfile will need to have the following contents for a community edition application:</p><pre data-language="dockerfile"><code><span class="hljs-comment"># The first instruction in a Dockerfile must be FROM, which selects a base image. We are using the image I published from a previous post about containerising the Mule ESB. Change this line to your own repository if you have created your own image.</span>
<span class="hljs-keyword">FROM</span> mikeyryan/mule:<span class="hljs-number">4.2</span>.<span class="hljs-number">0</span>-ce

<span class="hljs-comment"># Copy the jar that was generated during the package maven phase and place it in the apps folder</span>
<span class="hljs-keyword">COPY</span><span class="bash"> ./target/mule-4-hello-world*.jar /opt/mule/apps/</span>

<span class="hljs-comment"># Start the mule runtime</span>
<span class="hljs-keyword">CMD</span><span class="bash"> [<span class="hljs-string">"/opt/mule/bin/mule"</span>]</span>

<span class="hljs-comment"># HTTP listener default port</span>
<span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">8081</span>
</code></pre><p>If you are containerising an enterprise edition application, you will need to replace <code>-ce</code> with <code>-ee</code> in the line that begins with FROM. For applications other than the provided example from GitHub, you will need to modify the name of the JAR inside the COPY command to match the name of your project.</p><p>The above Dockerfile builds an image based on a pre-existing Mule ESB image and adds the application to it. Run it now by executing the following command:</p><pre data-language="bash"><code>docker build \
  --tag mule-4-hello-world .
</code></pre><p>This command creates a Docker image called <code>mule-4-hello-world</code> which we can use to run our application.</p><h2>Running the application using Docker</h2><p>To start a Docker container based on this image, execute the following command:</p><pre data-language="bash"><code>docker run --rm -it \
  --name mule-4-hello-world \
  -p 8081:8081 \
  mule-4-hello-world
</code></pre><p>This will start a Docker container which will run in the foreground. The Docker image exposes port 8081 and binds it to the same port on localhost.</p><p>Once the application is running, it can be accessed by navigating to <a href="http://localhost:8081/api/hello-world">http://localhost:8081/api/hello-world</a> in your browser. When accessed, your browser should show:</p><pre data-language="plaintext"><code>Hello from Mule 4.2.0
</code></pre><h2>Summary</h2><p>We have previously walked through how to containerise the Mule ESB and now we've demonstrated how easy it is to run Mule applications inside Docker containers. For those who prefer not to venture down the path of containerising the Mule ESB themselves, they can still containerise a Mule application using the mikeyryan/mule image to easily get up and running.</p><h2>References</h2><ul><li><a href="https://mike.gough.me/posts/docker/mule/esb/enterprise-edition "Containerising Mule Enterprise Service Bus (ESB) Enterprise Edition"">Containerising Mule Enterprise Service Bus (ESB) Enterprise Edition</a></li><li><a href="https://mike.gough.me/posts/docker/mule/esb/community-edition "Containerising Mule Enterprise Service Bus (ESB) Community Edition"">Containerising Mule Enterprise Service Bus (ESB) Community Edition</a></li><li><a href="https://github.com/Mike-Gough/mule-4-hello-world "Mike-Gough/mule-4-hello-world"">Mule 4 - Example hello world application repository</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/containerising-mule-esb-enterprise-edition</guid><title>Containerising Mule Enterprise Service Bus (ESB) Enterprise Edition</title><description>In this post we will assume that you have Docker and would like to create an image that contains the Enterprise Edition of the Mule ESB.</description><link>https://mike.gough.me/articles/containerising-mule-esb-enterprise-edition</link><pubDate>Mon, 27 May 2019 11:40:00 +0000</pubDate><content:encoded><![CDATA[<p>If you're looking for a Mule Docker image you can use without making your own, then you can check out <a href="https://hub.docker.com/r/mikeyryan/mule">mikeyryan/mule</a> on Docker Hub.</p><h2>Why Containerise the Mule ESB?</h2><img src="https://mike.gough.me//images/posts/mulesoft-logo.svg" alt="MuleSoft logo"/><p>The Mule ESB will run perfectly fine anywhere that you can install Java. However, you may find yourself in need of a solution that's more scalable than installing it on a standalone machine. A typical scenario for using the Mule ESB is to install it on a standalone server and then deploy all of your applications to it. This comes with a few disadvantages, primarily that a single unhealthy app can impact others in the same Mule ESB. One method of addressing this is to run multiple versions of the Mule ESB on the server, which is where Docker comes in.</p><p>With Docker we can easily deploy containers with the Mule ESB and a single application that will run in an independent isolated environment. This can also be useful for portability, as Docker will help us ensure that the application will run the same locally as it would when it's deployed to a server or the cloud.</p><h2>Creating a Docker image</h2><img src="https://mike.gough.me//images/posts/docker-logo.svg" alt="Docker logo"/><p>To begin with, we will need to create a new file called <code>Dockerfile</code> in our working directory. A Dockerfile has a file format that contains instructions and arguments, which define the contents and startup behaviour of the Docker container. To run the Mule ESB, our Dockerfile will need to contain the following:</p><pre data-language="dockerfile"><code><span class="hljs-comment"># The first instruction in a Dockerfile must be FROM, which selects a base image. Since it's recommended to use official Docker images, we will use the official image for openjdk.</span>
<span class="hljs-keyword">FROM</span> openjdk:<span class="hljs-number">8</span>-jdk

<span class="hljs-keyword">LABEL</span><span class="bash"> maintainer=<span class="hljs-string">"https://mike.gough.me"</span></span>

<span class="hljs-comment"># Define environment variables as arguments that can be passed in when building this image.</span>
<span class="hljs-keyword">ARG</span> MULE_VERSION=<span class="hljs-number">4.2</span>.<span class="hljs-number">0</span>
<span class="hljs-keyword">ARG</span> MULE_MD5=<span class="hljs-number">0</span>f098b4bbc65d27cee9af59904ed6545
<span class="hljs-keyword">ARG</span> TZ=Australia/Sydney

<span class="hljs-keyword">ENV</span> MULE_HOME=/opt/mule
<span class="hljs-keyword">ENV</span> MULE_DOWNLOAD_URL http://s3.amazonaws.com/new-mule-artifacts/mule-ee-distribution-standalone-${MULE_VERSION}.zip

<span class="hljs-comment"># Set the timezone</span>
<span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-variable">${TZ}</span> &gt; /etc/timezone</span>

<span class="hljs-comment"># Set the working directory</span>
<span class="hljs-keyword">WORKDIR</span><span class="bash"> <span class="hljs-variable">${MULE_HOME}</span></span>

<span class="hljs-keyword">RUN</span><span class="bash"> mkdir -p /opt &amp;&amp; \
    <span class="hljs-built_in">cd</span> /opt &amp;&amp; \
    wget <span class="hljs-string">"<span class="hljs-variable">$MULE_DOWNLOAD_URL</span>"</span> -O mule-ee-distribution-standalone-<span class="hljs-variable">${MULE_VERSION}</span>.zip</span>

<span class="hljs-comment"># Unpack Mule ESB</span>
<span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">cd</span> /opt &amp;&amp; \
  rm -rf mule-enterprise-standalone-<span class="hljs-variable">${MULE_VERSION}</span> &amp;&amp; \
  unzip mule-ee-distribution-standalone-<span class="hljs-variable">${MULE_VERSION}</span>.zip &amp;&amp; \
  rm -rf mule-standalone-<span class="hljs-variable">${MULE_VERSION}</span> &amp;&amp; \
  rm -f mule-ee-distribution-standalone-<span class="hljs-variable">${MULE_VERSION}</span>.zip &amp;&amp; \
  mv mule-enterprise-standalone-<span class="hljs-variable">${MULE_VERSION}</span> mule-standalone-<span class="hljs-variable">${MULE_VERSION}</span></span>

<span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">cd</span> /opt &amp;&amp; \
  rm -rf mule &amp;&amp; \
  ln -s mule-standalone-<span class="hljs-variable">${MULE_VERSION}</span> mule</span>

<span class="hljs-comment"># Set the mount locations</span>
<span class="hljs-keyword">VOLUME</span><span class="bash"> [<span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/logs"</span>, <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/conf"</span>, <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/apps"</span>, <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/domains"</span>, <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/patches"</span>, <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/.mule"</span>]</span>

<span class="hljs-comment"># Run this command on container start</span>
<span class="hljs-keyword">CMD</span><span class="bash"> [ <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/bin/mule"</span>]</span>

<span class="hljs-comment"># HTTP listener default port, remote debugger, JMX, MMC agent, AMC agent</span>
<span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">8081</span> <span class="hljs-number">5000</span> <span class="hljs-number">1098</span> <span class="hljs-number">7777</span> <span class="hljs-number">9997</span>
</code></pre><p>The above Docker file will create an image based on the official openjdk Docker image. It downloads and installs a specific version of the Mule ESB which can be passed as an optional argument when running the build process. To create the Docker image with version 4.2.0 of the Mule ESB, run the following command:</p><pre data-language="bash"><code>docker build \
  --build-arg MULE_VERSION=4.2.0 \
  -t mule:ee-4-2-0 .
</code></pre><p>With newer versions of Mule (4 and above) you may need to provide a licence instead of using a 30 day trial.</p><p>Thats it! In a future post we will look at how we can use this image as the base for another image which contains a Mule application.</p><h2>References</h2><ul><li><a href="https://hub.docker.com/r/mikeyryan/mule "mikeyryan/mule"">Docker Hub - Mule docker images</a></li><li><a href="https://docs.mulesoft.com/mule-runtime/4.2/mule-standalone "Mule Installation"">Mule Installation</a></li><li><a href="https://www.mulesoft.com/lp/dl/mule-esb-enterprise "Mule 4 standalone"">Mule 4 Standalone</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/containerising-mule-esb-community-edition</guid><title>Containerising Mule Enterprise Service Bus (ESB) Community Edition</title><description>In this post we will assume that you have Docker and would like to create an image that contains the Community Edition of the Mule ESB.</description><link>https://mike.gough.me/articles/containerising-mule-esb-community-edition</link><pubDate>Sun, 26 May 2019 08:35:00 +0000</pubDate><content:encoded><![CDATA[<p>If you're looking for a Mule Docker image you can use without making your own, then you can check out <a href="https://hub.docker.com/r/mikeyryan/mule">mikeyryan/mule</a> on Docker Hub.</p><h2>Why Containerise the Mule ESB?</h2><img src="https://mike.gough.me//images/posts/mulesoft-logo.svg" alt="MuleSoft logo"/><p>The Mule ESB will run perfectly fine anywhere that you can install Java. However, you may find yourself in need of a solution that's more scalable than installing it on a standalone machine. A typical scenario for using the Mule ESB is to install it on a standalone server and then deploy all of your applications to it. This comes with a few disadvantages, primarily that a single unhealthy app can impact others in the same Mule ESB. One method of addressing this is to run multiple versions of the Mule ESB on the server, which is where Docker comes in.</p><p>With Docker we can easily deploy containers with the Mule ESB and a single application that will run in an independent isolated environment. This can also be useful for portability, as Docker will help us ensure that the application will run the same locally as it would when it's deployed to a server or the cloud.</p><h2>Creating a Docker image</h2><img src="https://mike.gough.me//images/posts/docker-logo.svg" alt="Docker logo"/><p>To begin with, we will need to create file called <code>Dockerfile</code> in our working directory. A Dockerfile has a file format that contains instructions and arguments, which define the contents and startup behaviour of the Docker container. To run the Mule ESB, our Dockerfile will need to contain the following:</p><pre data-language="dockerfile"><code><span class="hljs-comment"># The first instruction in a Dockerfile must be FROM, which selects a base image. Since it's recommended to use official Docker images, we will use the official image for openjdk.</span>
<span class="hljs-keyword">FROM</span> openjdk:<span class="hljs-number">8</span>-jdk

<span class="hljs-keyword">LABEL</span><span class="bash"> maintainer=<span class="hljs-string">"https://mike.gough.me"</span></span>

<span class="hljs-comment"># Define environment variables as arguments that can be passed in when building this image.</span>
<span class="hljs-keyword">ARG</span> MULE_VERSION=<span class="hljs-number">4.2</span>.<span class="hljs-number">0</span>
<span class="hljs-keyword">ARG</span> TZ=Australia/Sydney

<span class="hljs-keyword">ENV</span> MULE_HOME=/opt/mule
<span class="hljs-keyword">ENV</span> MULE_DOWNLOAD_URL https://repository-master.mulesoft.org/nexus/content/repositories/releases/org/mule/distributions/mule-standalone/${MULE_VERSION}/mule-standalone-${MULE_VERSION}.tar.gz

<span class="hljs-comment"># Set the timezone</span>
<span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">echo</span> <span class="hljs-variable">${TZ}</span> &gt; /etc/timezone</span>

<span class="hljs-comment"># Set the working directory</span>
<span class="hljs-keyword">WORKDIR</span><span class="bash"> <span class="hljs-variable">${MULE_HOME}</span></span>

<span class="hljs-keyword">RUN</span><span class="bash"> mkdir -p /opt &amp;&amp; \
    <span class="hljs-built_in">cd</span> /opt &amp;&amp; \
    wget <span class="hljs-string">"<span class="hljs-variable">$MULE_DOWNLOAD_URL</span>"</span> -O mule-standalone-<span class="hljs-variable">${MULE_VERSION}</span>.tar.gz</span>

<span class="hljs-comment"># Unpack Mule ESB</span>
<span class="hljs-keyword">RUN</span><span class="bash"> tar xvzf /opt/mule-standalone-<span class="hljs-variable">${MULE_VERSION}</span>.tar.gz -C /opt &amp;&amp; \
  rm -f /opt/mule-standalone-<span class="hljs-variable">${MULE_VERSION}</span>.tar.gz</span>

<span class="hljs-keyword">RUN</span><span class="bash"> <span class="hljs-built_in">cd</span> /opt &amp;&amp; \
  rm -rf mule &amp;&amp; \
  ln -s mule-standalone-<span class="hljs-variable">${MULE_VERSION}</span> mule</span>

<span class="hljs-comment"># Set the mount locations</span>
<span class="hljs-keyword">VOLUME</span><span class="bash"> [<span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/logs"</span>, <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/conf"</span>, <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/apps"</span>, <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/domains"</span>, <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/patches"</span>, <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/.mule"</span>]</span>

<span class="hljs-comment"># Run this command on container start</span>
<span class="hljs-keyword">CMD</span><span class="bash"> [ <span class="hljs-string">"<span class="hljs-variable">${MULE_HOME}</span>/bin/mule"</span>]</span>

<span class="hljs-comment"># HTTP listener default port, remote debugger, JMX, MMC agent, AMC agent</span>
<span class="hljs-keyword">EXPOSE</span> <span class="hljs-number">8081</span> <span class="hljs-number">5000</span> <span class="hljs-number">1098</span> <span class="hljs-number">7777</span> <span class="hljs-number">9997</span>
</code></pre><p>The above Docker file will create an image based on the official openjdk Docker image. It downloads and installs a specific version of the Mule ESB which can be passed as an optional argument when running the build process. To create the Docker image with version 4.2.0 of the Mule ESB, run the following command:</p><pre data-language="bash"><code>docker build \
  --build-arg MULE_VERSION=4.2.0 \
  -t mule:ce-4-2-0 .
</code></pre><p>Thats it! In a future post we will look at how we can use this image as the base for another image which contains a Mule application.</p><h2>References</h2><ul><li><a href="https://hub.docker.com/r/mikeyryan/mule "mikeyryan/mule"">Docker Hub - Mule docker images</a></li><li><a href="https://docs.mulesoft.com/mule-runtime/4.2/mule-standalone "Mule Installation"">Mule Installation</a></li><li><a href="https://developer.mulesoft.com/download-mule-esb-runtime "Mule Kernel"">Mule Kernel</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/github-actions</guid><title>Continuous Deployment with GitHub Actions and Docker Hub</title><description>In this post we will assume that you practice Continuous Integration (CI) and have a product which is packaged as a Docker image. As your next step you are looking to implement Continuous Deployment (CD) from scratch or move to it from a Continuous Delivery workflow. Our aim will be to build and push a docker image to Docker Hub using GitHub actions.</description><link>https://mike.gough.me/articles/github-actions</link><pubDate>Mon, 20 May 2019 16:25:00 +0000</pubDate><content:encoded><![CDATA[<h2>Principles</h2><p>Before we dive into how we can achieve this goal, it’s important to understand the principles behind what we are trying to achieve and why. If you’re already familiar with Continuous Delivery and Continuous Deployment as well as the distinction between the two, feel free to skip this next part.</p><h3>What is Continuous Delivery?</h3><p>Continuous Delivery is a software development discipline where you build software in such a way that the it can be released to production at any time<sup>[1]</sup>. In a Continuous Delivery workflow, each development change that is pushed to the main repository is ready to be shipped. However, the action of shipping requires human approval. Although there is usually a focus on automated testing as part of this process, in many organisations the risk of promoting a release to production is shouldered by the individual approving that release. Onus is placed on the developers to prioritise keeping the code deliverable over implementing new features.</p><h3>How does Continuous Delivery differ from Continuous Deployment?</h3><p>By contrast, in Continuous Deployment each development change that is pushed to the main repository is automatically released to production, without any human intervention. In this workflow, a strong emphasis is placed on automated testing, as it should not be possible to merge code into the main development branch without that code passing a test suite. This means that the quality of your test suite determines the level of risk for a release, and automated testing must be prioritised during development. As such it’s important to ensure you don’t fall into the trap of mistaking good code coverage in your test suite for good quality tests. Developers within the team must ensure that the quality of tests presented in code reviews remains high.</p><h2>Using Github actions for Continuous Deployment</h2><p>Now that we understand what Continuous Deployment is and what we are aiming for, let’s create a workflow that builds a Docker image and publishes it to Docker Hub. To begin, you’ll need to navigate to GitHub, ensure you are logged in and have opened the repository that you would like to work with. The repository should already contain a <code>Dockerfile</code>. Click the <em>Actions</em> button at the top of the page: <img src="https://mike.gough.me//images/posts/github-actions-title-bar.jpg" alt="GitHub actions button"/></p><p>GitHub will prompt you to confirm that you’d like to create a new workflow, click the <em>Create a new workflow</em> button: <img src="https://mike.gough.me//images/posts/github-actions-create-button.jpg" alt="Git hook screenshot"/></p><p>Leave the name of the file as <code>main.workflow</code> and click the button labelled <em>Edit file</em> <img src="https://mike.gough.me//images/posts/github-actions-heading.jpg" alt="Git hook screenshot"/></p><p>Add the following to the contents of the editor, replacing <code>&lt;project-name&gt;</code> and <code>&lt;docker-hub-username&gt;</code> with the name of your project and username for Docker Hub:</p><pre data-language="plaintext"><code># Create a new workflow that’s triggered by a push to master
workflow "Build on push" {
  on = "push"
  resolves = [
    "Push Docker image with build number",
    "Push Docker image with latest",
    "Archive release"
  ]
}

# Optionally add an action to run your test suite here

# Filter pushes to only those on the master branch
action "Filter for master" {
  uses = "actions/bin/filter@master"
  args = "branch master"
}

# Login to Docker Hub with your credentials (The GitHub UI will prompt you for them if they have not already been provided).
action "Authenticate with Docker Registry" {
  uses = "actions/docker/login@master"
  needs = ["Filter for master"]
  secrets = ["DOCKER_USERNAME", "DOCKER_PASSWORD"]
}

# Build a docker image based off of a Dockerfile in the root directory of the repository
action "Build Docker Image" {
  uses = "actions/docker/cli@8cdf801b322af5f369e00d85e9cf3a7122f49108"
  needs = ["Authenticate with Docker Registry"]
  args = "build -f Dockerfile --tag &lt;project-name&gt; ."
}

# Give the docket image a unique tag based on the GitHub SHA
action "Tag Docker Image with build number" {
  uses = "actions/docker/cli@8cdf801b322af5f369e00d85e9cf3a7122f49108"
  needs = ["Build Docker Image"]
  args = "tag &lt;project-name&gt; &lt;docker-hub-username&gt;/&lt;project-name&gt;:$GITHUB_SHA"
}

# Automatically push the image to Docker Hub
action "Push Docker image with build number" {
  uses = "actions/docker/cli@8cdf801b322af5f369e00d85e9cf3a7122f49108"
  needs = ["Tag Docker Image with build number"]
  args = "push &lt;docker-hub-username&gt;/&lt;project-name&gt;:$GITHUB_SHA"
}

# Filter for a tag which indicates that this push is a release (i.e. the push is tagged with v1.0.0)
action "Filter for tag" {
  uses = "actions/bin/filter@master"
  needs = ["Push Docker image with build number"]
  args = "tag v*"
}

# Tag the docket image as latest
action "Tag Docker Image with latest" {
  uses = "actions/docker/cli@8cdf801b322af5f369e00d85e9cf3a7122f49108"
  needs = ["Filter for tag"]
  args = "tag &lt;project-name&gt; &lt;docker-hub-username&gt;/&lt;project-name&gt;:latest"
}

# Automatically push the image tagged latest to Docker Hub
action "Push Docker image with latest" {
  uses = "actions/docker/cli@8cdf801b322af5f369e00d85e9cf3a7122f49108"
  needs = ["Tag Docker Image with latest"]
  args = "push &lt;docker-hub-username&gt;/&lt;project-name&gt;:latest"
}

# Create a release ZIP archive and add it to the repository
action "Archive release" {
  uses = "lubusIN/actions/archive@master"
  needs = ["Filter for tag"]
  env = {
    ZIP_FILENAME = "&lt;project-name&gt;"
  }
}
</code></pre><p>That was a lot to digest, so let’s take a look at the actions in this workflow. The workflow is trigged by any push to the repository. The first action filters out all pushes other than those to the master branch. It then attempts to login to Docker Hub using the credentials you have supplied as secrets.</p><p>Once authenticated, an action builds a Docker image, another tags it and yet another pushes it to Docker Hub. The next action is a filter, it checks if the push to the master branch was tagged as a release. If the push was a release then the next action tags the Docker image as latest and another pushes it to Docker Hub. Finally, an action zips the source code up and publishes it to the release page on GitHub.</p><p>To see all of this in action, commit the <code>main.workflow</code> file to the repository. Navigating to the actions tab should now show any in-progress builds as well as historical ones:</p><img src="https://mike.gough.me//images/posts/github-actions-run-results.jpg" alt="Git hook screenshot"/><h2>References</h2><ul><li><a href="https://martinfowler.com/bliki/ContinuousDelivery.html        "Martin Fowler - Continuous Delivery"">Martin Fowler - Continuous Delivery</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/git-hooks</guid><title>Git pre-commit hook for Linting RESTful API Modelling Language (RAML)</title><description>Continuous Integration (CI) is a practice that requires developers to push code into a shared repository several times a day. When pushing our code to a shared repository, we should strive to ensure that our code is syntactically correct and builds so that other developers can grab the latest copy and begin iterating upon it easily. Today we are going to look at this principle and how we can easily identify and correct common coding mistakes when designing Service Contracts for Application Programming Interfaces (APIs). Specifically, we will explore how we can use Git hooks as a mechanism for ensuring a RAML Service Contract is valid using a Linter called RAML Enforcer.</description><link>https://mike.gough.me/articles/git-hooks</link><pubDate>Sat, 18 May 2019 09:45:00 +0000</pubDate><content:encoded><![CDATA[<h2>What is RAML Enforcer?</h2><p><em>RAML Enforcer</em> is a Linter which can be used to examine source code for programatic errors, bugs, stylistic errors or suspicious patterns. In a CI toolchain, <em>Linting</em> is performed very early in the workflow, usually prior to running unit tests or integrating code into a shared repository. <em>RAML Enforcer</em> is configurable through command line arguments and provides developers with the option to select from a set of prepackaged rules to enforce coding standards. In this case, we are interested in automatically checking our Service Contract for programatic errors and bugs prior to committing any changes.</p><h2>How can RAML Enforcer be run automatically?</h2><p>Git has a mechanism for triggering scripts when interesting events occur, called hooks. There are two main categories of hooks, client side and server side. To address our goal of checking the quality of our RAML before integrating it into our local version control repository, we will look at using a local pre-commit hook. So.. what is a pre-commit hook, you ask? A pre-commit hook is a script that Git executes before committing staged files in the repository. In this case it will allow us to inspect the snapshot that’s about to be committed and see if it meets our code quality standards. Pre-commit hooks reside in local repositories within the <code>.git/hooks</code> directory.</p><p>Assuming that you already have a Git repository that contains a RAML Service Contract that you’d like to <em>Lint</em>, let's begin by navigating to the <code>.git/hooks</code> directory and creating a file called <code>pre-commit</code><code></code>. Add the following contents to the file, replacing <code>&lt;main-raml-file-path&gt;</code> with the path to your main RAML file:</p><pre data-language="bash"><code><span class="hljs-meta">#!/bin/sh</span>
<span class="hljs-built_in">echo</span> <span class="hljs-string">"# Running RAML Enforcer"</span>
sudo docker run \
  --init --rm \
  --volume $(<span class="hljs-built_in">pwd</span>):/tmp <span class="hljs-string">"mikeyryan/raml-enforcer:latest"</span> \
  /tmp/&lt;main-raml-file-path&gt;
</code></pre><p>Hooks need to be executable, so you may need to change the file permissions of <code>pre-commit</code> if you're creating it from scratch. You can do so by running the following command:</p><pre data-language="bash"><code>chmod +x pre-commit
</code></pre><h2>Seeing it in action</h2><p>Now that we have a pre-commit hook in place for our repository, lets see how it looks! Make a change to your RAML Service Contract, then try and commit it. You should see a report like this printed to the screen:</p><img src="https://mike.gough.me//images/posts/git-hook-linter-commit.svg" alt="Git hook screenshot"/><p>When <em>RAML Enforcer</em> detects that the Service Contract contains errors or does not meet code standards, it prevents your staged files from being committed.</p><h2>Further reading</h2><ul><li><a href="https://www.atlassian.com/git/tutorials/git-hooks">Atlassian - Git hooks</a></li><li><a href="https://git-scm.com/book/en/v2/Customizing-Git-Git-Hooks">Customising Git - Git Hooks</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/npm-script-docker-image</guid><title>Containerising a Node.js script</title><description>When working inside a Continuous Integration (CI) and Continuous Delivery (CD) environment, portability of code is often a core concern that needs to be addressed. Developers write code locally and need some level of assurance that it will run consistently regardless of where it is deployed. This is an area where Docker shines. The goal of this post is to run a script, written with Node.js, inside a docker container. It assumes that you have an existing script which requires access to files on the local file system as well as an account on Docker Hub.</description><link>https://mike.gough.me/articles/npm-script-docker-image</link><pubDate>Fri, 17 May 2019 19:11:00 +0000</pubDate><content:encoded><![CDATA[<h2>Docker concepts</h2><p>To begin with, it's important to understand what Docker is and the principles behind it. Docker is a platform for <em>developers</em> and <em>system administrators</em> to develop, deploy and run applications with containers. The use of containers to deploy applications is called containerization, which is popular in CI and CD workflows because containers are<sup>[1]</sup>:</p><ul><li>Flexible - Any application can be containerized</li><li>Lightweight - They leverage and share the host kernel</li><li>Interchangeable - You can deploy updates and upgrades with zero down time</li><li>Portable - You can build locally and deploy to a server on premises or in the cloud</li><li>Scalable - You can scale your containers horizontally; increasing, decreasing or distributing replicas of them automatically</li><li>Stackable - You can stack your containers vertically, defining a stack declaratively</li></ul><p>In Docker a container is launched by running an image, and an image is an executable package that includes everything needed to run an application.</p><h2>Great.. so how do we create an image?</h2><p>To begin with, we will need to create file called <code>Dockerfile</code> in our working directory. A Dockerfile has a file format that contains instructions and arguments, which define the contents and startup behaviour of the Docker container. To run a Node.js script, our Dockerfile will need to contain the following, replacing <code>&lt;script-name&gt;</code> with the filename of your script:</p><pre data-language="dockerfile"><code><span class="hljs-comment"># The first instruction in a Dockerfile must be FROM, which selects a base image. Since it's recommended to use official Docker images, we will use the official image for node. We will chose a specific image rather than defaulting to latest as future node versions may break our application.</span>
<span class="hljs-keyword">FROM</span> node:<span class="hljs-number">12</span>-alpine

<span class="hljs-comment"># Sets the working directory to /usr/src/app.</span>
<span class="hljs-keyword">WORKDIR</span><span class="bash"> /usr/src/app</span>

<span class="hljs-comment"># Copies the package file for NPM to the working directory.</span>
<span class="hljs-keyword">COPY</span><span class="bash"> package*.json ./</span>

<span class="hljs-comment"># Installs the required NPM packages.</span>
<span class="hljs-keyword">RUN</span><span class="bash"> npm install</span>

<span class="hljs-comment"># Copies the application from the current directory to the working directory of the image.</span>
<span class="hljs-keyword">copy</span><span class="bash"> . .</span>

<span class="hljs-comment"># If an action does not use the runs configuration option, the commands in ENTRYPOINT will execute. The Docker ENTRYPOINT instruction has a shell form and exec form. We will use the exec form of the ENTRYPOINT instruction to call our node script. This will allow us to pass arguments to the script when we run the container.</span>
<span class="hljs-keyword">ENTRYPOINT</span><span class="bash"> [<span class="hljs-string">"node"</span>, <span class="hljs-string">"&lt;script-name&gt;"</span>]</span>
</code></pre><p>So the above <code>Dockerfile</code> will create a new image based on the official Node image, install our scripts NPM dependancies, copy the script to a working directory and when a container is started, automatically run the script. One thing we have overlooked is that since the <code>Dockerfile</code> installs NPM dependancies for us we shouldn't copy any node_modules into the image. To avoid this we can create a file called <code>.dockerignore</code> with the following contents:</p><pre data-language="plaintext"><code>node_modules
</code></pre><p>With our <code>Dockerfile</code> and <code>.dockerignore</code> files in place, we can now build our image by running the following command and replacing <code>&lt;docker-hub-username&gt;</code> with your Docker Hub username and <code>&lt;image-name&gt;</code> with something memorable:</p><pre data-language="bash"><code>sudo docker build \
  --no-cache \
  --tag <span class="hljs-string">"&lt;docker-hub-username&gt;/&lt;image-name&gt;:latest"</span> .
</code></pre><img src="https://mike.gough.me//images/posts/npm-script-docker-build.svg" alt="Docker build screenshot"/><h2>Running our image</h2><p>Having built a Docker image, we can now run it locally by using the following command and replacing <code>&lt;docker-hub-username&gt;</code> with your Docker Hub username and <code>&lt;image-name&gt;</code> with the name used earlier:</p><pre data-language="bash"><code>sudo docker run \
  --init --rm \
  --volume $(<span class="hljs-built_in">pwd</span>):/tmp \
  <span class="hljs-string">"&lt;docker-hub-username&gt;/&lt;image-name&gt;:latest"</span> &lt;image-args&gt;
</code></pre><img src="https://mike.gough.me//images/posts/npm-script-docker-run.svg" alt="Docker run screenshot"/><p>For reference, the arguments we are passing through to Docker are:</p><ul><li><strong>init</strong>: sets the ENTRYPOINT to tini. tini is a lightweight init process which will help ensure that Node.js returns and responds to signals correctly</li><li><strong>rm</strong>: removes the container once it has finished running</li><li><strong>volume</strong>: mounts the specified location inside the container</li></ul><h2>Pushing our image to Docker Hub</h2><p>The final step of our journey is to publish our local docker image to Docker Hub so that we are able to pull and run it from other locations such as a CI/CD server. Before we can push our image we will need to login do Docker Hub by running the following command and entering our Docker Hub username and password when prompted:</p><pre data-language="bash"><code>docker login
</code></pre><p>Once successfully authenticated, we can push our image by running the following command and replacing <code>&lt;docker-hub-username&gt;</code> with your Docker Hub username and <code>&lt;image-name&gt;</code> with the name used earlier:</p><pre data-language="bash"><code>sudo docker \
  push <span class="hljs-string">"&lt;docker-hub-username&gt;/&lt;image-name&gt;:latest"</span>
</code></pre><img src="https://mike.gough.me//images/posts/npm-script-docker-push.svg" alt="Docker push screenshot"/><p>Thats it! You can navigate to <code>https://hub.docker.com/r/&lt;docker-hub-username&gt;/&lt;image-name&gt;</code> to see your published image.</p><h2>References</h2><ul><li><a href="https://docs.docker.com/get-started/        "Docker"">Docker - Get Started</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/kubernetes-with-dashboard</guid><title>Setup Kubernetes and its dashboard on Docker Desktop</title><description>In this post we will walk through how you can install and run a single node Kubernetes instance using Docker Desktop Community Edition for Windows or Mac.</description><link>https://mike.gough.me/articles/kubernetes-with-dashboard</link><pubDate>Thu, 16 May 2019 21:41:00 +0000</pubDate><content:encoded><![CDATA[<h2>Prerequisites</h2><p>To keep things simple, we will assume you have access to a bash Command Line Interface (CLI) on your local machine. Mac users should have access to the <em>Terminal</em> application while Windows users will need to install one of several options, including but not limited to:</p><ul><li><a href="https://docs.microsoft.com/en-us/windows/wsl/install-win10">Windows subsystem for Linux</a></li><li><a href="https://gitforwindows.org">Git for Windows (Git Bash)</a></li></ul><h2>What is Kubernetes</h2><p>Kubernetes (K8s) is a portable open source platform for automating the deployment, scaling and management of containerised applications. The name Kubernetes originates from the Greek language, meaning helmsman or pilot and is the root of the words governor and cybernetic. It emerged from the need for declarative configuration for the orchestration of containers, which had just been popularised by Docker. Although Docker provided a simple way to package, distributed and deploy applications, its focus on single machine left a gap for management at enterprise or cloud scales.</p><p>K8s can be thought of as:</p><ul><li>a container platform;</li><li>a microservices platform; and</li><li>a portable cloud platform.</li></ul><p>It's ability to orchestrate computing, networking, and storage infrastructure on behalf of user workloads makes it particularly well suited for hosting distributed applications such as an Enterprise Service Bus (ESB).</p><h2>Install Docker Desktop</h2><img src="https://mike.gough.me//images/posts/docker-logo.svg" alt="Docker logo"/><p>If you want to get started with Kubernetes on a Windows 10 or Mac OS operating system, Docker Desktop is the quickest way. Docker Desktop comes in two editions, a free community edition and a paid enterprise edition. It includes everything you need to build, test and ship containerised applications right from your machine and for this walkthrough, either edition will be suitable. To obtain a copy, head to the <a href="https://www.docker.com/products/docker-desktop">Docker website</a>, and follow the instructions to download and install it.</p><p>Once installed, you can verify that Docker is up and running by opening your CLI of choice and executing the following command:</p><pre data-language="bash"><code>docker version
</code></pre><p>If successful, you should seen some output printed to the screen similar to this:</p><pre data-language="plaintext"><code>Client: Docker Engine - Community
 Version:           18.09.2
 API version:       1.39
 Go version:        go1.10.8
 Git commit:        6247962
 Built:             Sun Feb 10 04:12:39 2019
 OS/Arch:           darwin/amd64
 Experimental:      false

Server: Docker Engine - Community
 Engine:
  Version:          18.09.2
  API version:      1.39 (minimum version 1.12)
  Go version:       go1.10.6
  Git commit:       6247962
  Built:            Sun Feb 10 04:13:06 2019
  OS/Arch:          linux/amd64
  Experimental:     true
 Kubernetes:
  Version:          v1.10.11
  StackAPI:         v1beta2
</code></pre><h2>Install Kubernetes</h2><img src="https://mike.gough.me//images/posts/kubernetes-logo.svg" alt="Kubernetes logo"/><p>Once Docker Desktop is installed, you should see a whale icon in the taskbar for Windows or the menu bar for Mac. Clicking the icon should reveal a content menu with an option for <em>Settings</em> or <em>Preferences</em> depending on your Operating System, select it. Once open, navigate to the Kubernetes tab. Kubernetes is not installed by Docker Desktop by default, you'll need to check the <em>Enable Kubernetes</em> checkbox and wait for it to be downloaded and installed before proceeding. The amount of time this will take largely depends on your internet speed, once the window shows <em>Kubernetes is running</em>, you should be ready to continue.</p><p>Once enabled, you can verify that Kubernetes is available by opening your CLI of choice and executing the following command:</p><pre data-language="bash"><code>kubectl version
</code></pre><p>If successful, you should seen some output printed to the screen similar to this:</p><pre data-language="plaintext"><code>Client Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.11", GitCommit:"637c7e288581ee40ab4ca210618a89a555b6e7e9", GitTreeState:"clean", BuildDate:"2018-11-26T14:38:32Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"10", GitVersion:"v1.10.11", GitCommit:"637c7e288581ee40ab4ca210618a89a555b6e7e9", GitTreeState:"clean", BuildDate:"2018-11-26T14:25:46Z", GoVersion:"go1.9.3", Compiler:"gc", Platform:"linux/amd64"}
</code></pre><p>This will have resulted in Docker Desktop setting up a single node Kubernetes cluster on your local machine.</p><h2>Installing the Kubernetes Dashboard</h2><p>Dashboard is an official web-based user interface for Kubernetes. It can be used as an alternative to the CLI for those that prefer to work with a Graphical User Interface (GUI). At a high level, it features the ability to deploy, troubleshoot and manage resources for Kubernetes clusters. In particular, it can be used to get an overview of running applications and to perform common tasks such as scaling a deployment, initiate a rolling update, restarting a pod or deploying new applications.</p><p>Although the Dashboard is the official GUI for Kubernetes, it is not deployed by default. To deploy it to your single node Kubernetes cluster, run the following command:</p><pre data-language="bash"><code>kubectl apply \
  -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta8/aio/deploy/recommended.yaml
</code></pre><p>To make the deployed Dashboard accessible, use the kubectl command-line tool by running the following command:</p><pre data-language="bash"><code>kubectl proxy
</code></pre><p>You will then be able to navigate to <a href="http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/">http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/</a> in your browser to access the dashboard.</p><h2>Setting up a Dashboard user</h2><p>Navigating to the Dashboard will have presented a screen asking for a Kubeconfig file or an access token. To use the Dashboard, you'll need to create a user and obtain a bearer token for them. Begin by creating a file called <code>dashboard-admin-user.yaml</code> with the following contents:</p><pre data-language="yaml"><code><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceAccount</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">admin-user</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>
</code></pre><p>Then navigate to the location you saved the file in your CLI and use the following command to create the admin user:</p><pre data-language="bash"><code>kubectl apply \
  -f dashboard-admin-user.yaml
</code></pre><p>Finally, to obtain a bearer token, you'll need to run the following command:</p><pre data-language="bash"><code>kubectl -n kube-system describe secret \
  $(kubectl -n kube-system get secret | grep admin-user | awk <span class="hljs-string">'{print $1}'</span>)
</code></pre><p>It should print some output to the screen like this:</p><pre data-language="plaintext"><code>Name:         admin-user-token-m9nr5
Namespace:    kube-system
Labels:       &lt;none&gt;
Annotations:  kubernetes.io/service-account.name=admin-user
              kubernetes.io/service-account.uid=b9b37a57-812d-11e9-9877-025000000001

Type:  kubernetes.io/service-account-token

Data
====
ca.crt:     1025 bytes
namespace:  11 bytes
token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlLXN5c3RlbSIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLW05bnI1Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJiOWIzN2E1Ny04MTJkLTExZTktOTg3Ny0wMjUwMDAwMDAwMDEiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZS1zeXN0ZW06YWRtaW4tdXNlciJ9.EZ37ihKfHBszN1Ujz6EgDj143Q-hUKtWUyo1s88D0-WCwdtnmdZLUyvg4d5H6NmZo7AbrkPrNHXOLy45piU9ghQHejicz5SBLI_JtPFO68BxOiGv7MuNYAAHJO82y-NNbTDjfx6Rbj9RK5-pWoHO9eOSvHa-XDC0OH3Usj4gjjVdXhf5uBl3meKPtUwlaXX0ziaIMoDmfHLw43vqugpJDyNMXkcil0s0NzFFlPBLu4enPx_TEuJp0pKsBEKXDNgB9amSmljD7ovdNd9ocIA7kNBe3SSctTkqxYqOrABuaC3KDmCGzCOGRboDUJEL8FP3HtbimsXCm8jXKzqo-5a_YA
</code></pre><h2>Accessing the Dashboard</h2><p>Copy the token that was printed to the screen, navigate back to the browser window we opened earlier and select the option <em>Token</em>. Paste the token into the password field shown on the screen and press the <em>Sign in</em> button. That’s it! You should now be able to access Kubernetes Dashboard as shown below:</p><img src="https://mike.gough.me//images/posts/kubernetes-dashboard.png" alt="Kubernetes Dashboard"/><h2>References</h2><ul><li><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/ "Web UI (Dashboard)"">Web UI (Dashboard)</a></li><li><a href="https://github.com/kubernetes/dashboard/wiki/Creating-sample-user "Creating sample user"">Creating sample user</a></li><li><a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/ "What is kubernetes"">What is Kubernetes</a></li></ul>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/linting-RAML</guid><title>Linting RESTful API Modelling Language (RAML)</title><description>Within a Continuous Integration (CI) and Continuous Delivery (CD) environment, the first principles is that no code is delivered without automated tests. Today we are going to look at this principle and how we can easily identify and correct common  coding mistakes when designing Service Contracts for Application Programming Interfaces (APIs). Specifically, we will explore how Linting can be used for this purpose.</description><link>https://mike.gough.me/articles/linting-RAML</link><pubDate>Thu, 16 May 2019 21:41:00 +0000</pubDate><content:encoded><![CDATA[<h2>What is Linting</h2><p><em>Linting</em> is the process of using a tool to examine source code for programatic errors, bugs, stylistic errors or suspicious patterns. A <em>Linter</em> or <em>Lint</em> is a piece of software that supports verifying code quality through <em>Linting</em>. Most Linters are highly configurable and extensible, allowing developers to select from a set of prepackaged rules to enforce a particular coding standard, or to invent their own rules as needed.</p><p>In a CI toolchain, <em>Linting</em> is performed very early in the workflow, usually prior to running unit tests. It is often implemented on the local development machine inside of a pre-commit in distributed Version Control Systems such as <em>Git</em> to reject code that does not pass the <em>Linter</em> code quality checks. It can also be enforced as part of stage within the build process.</p><h2>Where does Linting come from?</h2><p><em>Lint</em> was the name of a program written in 1978 by Stephen Johnson at Bell Labs to identify problems with C source code before complication and runtime. It was later used in 1979 inside the seventh version of the Unix operating system. Over time the term <em>Lint</em> became a verb that meant checking your source code.</p><h2>So... why Lint your Service Contracts?</h2><p>Whether you're implementing a Service Layer, Enterprise Service Bus or Application Network, as your API progresses through the various stages of development, code quality becomes critical. Collaborating with your customers to ensure that the design of an API meets functional requirements is a given, but it's also important to ensure that it doesn't include any structural issues. A poorly structured API can impact the reliability and efficiency of its consumers as well as make the implementation harder. For those employ a micro-services architecture or find themselves working with large teams, a consistent approach to the design of APIs is particularly important for maintainability.</p><p><em>Linting</em> can be used to find and resolve both functional and structural issues with an API'cs Service Contract. It can identify and correct common code mistakes without having to run your code or execute tests. Some of the key benefits a <em>Linter</em> provides you with include:</p><ul><li>Avoiding errors - they give you immediate feedback about things that look like errors or could potentially be dangerous</li><li>Readability - they can drive you to write cleaner and more constant code</li><li>Maintainability - they can be used to help developers working as part of a team to adhere to a uniform coding standards</li><li>Portability - they can be used within an IDE, text editors such as Visual Studio Code, a command line and continuous integration tools</li><li>Automatic fixes - using a prettier they can enabling code style issues to be fixed automatically</li></ul><h2>Integrating Linting into your RAML Service Contract</h2><p>Unfortunately, at the time of writing most of the RAML <em>Linters</em> available are just wrapping an open source parser. Parsing RAML helps to identify errors, but does little in the way of improving the readability or maintainability of your Service Contracts. This is why I decided to build my own <em>Linter</em>, called <em>RAML Enforcer</em>. <em>RAML Enforcer</em> is a command line tool for identifying and reporting on patterns found within RAML code. It supports, RAML 0.8, RAML 1.0, Includes and Fragments.</p><p>Although there are a few different options for running <em>RAML Enforcer</em>, in this case we will build and run it directly from the source code. To get up and running you'll need to ensure you have Git, Node.js and NPM installed on your local machine. Once you have them, you can begin by opening up your Command Line Interface (CLI) of choice and using it to clone the source code of the project from GitHub:</p><pre data-language="bash"><code>git <span class="hljs-built_in">clone</span> https://github.com/Mike-Gough/raml-enforcer
</code></pre><p>Next, you will need to navigate to the repository we just cloned and install the required dependancies for <em>RAML Enforcer</em> by running the following command:</p><pre data-language="bash"><code><span class="hljs-built_in">cd</span> raml-enforcer &amp;&amp; \
  npm install
</code></pre><p>Finally, you can execute the <em>Linter</em> by using the following command, replacing <code>main-api-file</code> with the path to the main RAML file for your service contract:</p><pre data-language="bash"><code>node raml-enforcer.js &lt;main-api-file&gt;
</code></pre><p>The result will be a report which identifies errors and highlights styling issues:</p><img src="https://mike.gough.me//images/posts/raml-enforcer-report.svg" alt="RAML Enforcer Report"/><p>In a future post, we will look at how <em>RAML Enforcer</em> can be used in a pre-commit hook.</p>]]></content:encoded></item><item><guid isPermaLink="true">https://mike.gough.me/articles/hello-world</guid><title>Hello World</title><description>The internet is full of how-to guides for architects and software developers, this blog is not one of them.</description><link>https://mike.gough.me/articles/hello-world</link><pubDate>Wed, 15 May 2019 17:17:00 +0000</pubDate><content:encoded><![CDATA[<p>I offer by way of explanation, a tale of integrating software. I intend to use this space to document the many projects and prototypes I work on for later reference.</p><p>I've chosen to make it public so that it might be of some benefit to other architects, software developers and technologists. Along the way I'll share my endeavour to make something useful and lasting whilst being stymied by obstacles both old and new.</p>]]></content:encoded></item></channel></rss>